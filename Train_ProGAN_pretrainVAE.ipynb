{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Input,AveragePooling2D, Concatenate, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop,SGD\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array,array_to_img\n",
    "import keras\n",
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:1\", \"/gpu:2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "# tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return np.asarray(images_list)\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    "    # Convert image to 0-255\n",
    "    images1 = ((images1*127.5)+127.5).astype('float32')\n",
    "    #images2 = ((images2*127.5)+127.5).astype('float32')\n",
    "    # Scale image to 299x299x3\n",
    "    images1 = scale_images(images1, (299,299,3))\n",
    "    #images2 = scale_images(images2, (299,299,3))\n",
    "    # pre-process images\n",
    "    images1 = preprocess_input(images1)\n",
    "    images2 = preprocess_input(images2)\n",
    "    # calculate activations\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "# prepare the inception v3 model\n",
    "model_inceptionv3 = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(PixelNormalization,self).__init__(**kwargs)\n",
    "    def call(self,x):\n",
    "        value = x**2\n",
    "        mean = K.mean(value, axis = -1, keepdims=True)\n",
    "        x = x/K.sqrt(mean + 1.0e-8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(tf.keras.layers.Add):\n",
    "    # init with default value\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = K.variable(alpha, name='ws_alpha')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                'alpha' : 0.0\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    # output a weighted sum of inputs\n",
    "    def _merge_function(self, inputs):\n",
    "        # only supports a weighted sum of two inputs\n",
    "        assert (len(inputs) == 2)\n",
    "        # ((1-a) * input1) + (a * input2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStdev(tf.keras.layers.Layer):\n",
    "    # initialize the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinibatchStdev, self).__init__(**kwargs)\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        # calculate the mean value for each pixel across channels\n",
    "        mean = K.mean(inputs, axis=0, keepdims=True)\n",
    "        # calculate the squared differences between pixel values and mean\n",
    "        squ_diffs = K.square(inputs - mean)\n",
    "        # calculate the average of the squared differences (variance)\n",
    "        mean_sq_diff = K.mean(squ_diffs, axis=0, keepdims=True)\n",
    "        # add a small value to avoid a blow-up when we calculate stdev\n",
    "        mean_sq_diff += 1e-8\n",
    "        # square root of the variance (stdev)\n",
    "        stdev = K.sqrt(mean_sq_diff)\n",
    "        # calculate the mean standard deviation across each pixel coord\n",
    "        mean_pix = K.mean(stdev, keepdims=True)\n",
    "        # scale this up to be the size of one input feature map for each sample\n",
    "        shape = K.shape(inputs)\n",
    "        output = K.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
    "        # concatenate with the output\n",
    "        combined = K.concatenate([inputs, output], axis=-1)\n",
    "        return combined\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # create a copy of the input shape as a list\n",
    "        input_shape = list(input_shape)\n",
    "        # add one to the channel dimension (assume channels-last)\n",
    "        input_shape[-1] += 1\n",
    "        # convert list to a tuple\n",
    "        return tuple(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "    def __init__(self, batch_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        alpha = tf.random.uniform((self.batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0][0]) + ((1 - alpha) * inputs[1][0])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'batch_size': self.batch_size,\n",
    "            \n",
    "        })\n",
    "        return config\n",
    "    \n",
    "def gradient_penalty(y_true, y_pred, discriminator,gradient_penalty_weight):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "      \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(y_pred)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = discriminator(y_pred, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [y_pred])[0]\n",
    "        # 3. Calcuate the norm of the gradients\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp*gradient_penalty_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientPenalty(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        (target, wrt) = inputs\n",
    "        grad = K.gradients(target, wrt)[0]\n",
    "        return K.sqrt(K.sum(K.batch_flatten(K.square(grad)),\n",
    "            axis=1, keepdims=True))-1\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (input_shapes[1][0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "class Cut_VGG19:\n",
    "    \"\"\"\n",
    "    Class object that fetches keras' VGG19 model trained on the imagenet dataset\n",
    "    and declares <layers_to_extract> as output layers. Used as feature extractor\n",
    "    for the perceptual loss function.\n",
    "    Args:\n",
    "        layers_to_extract: list of layers to be declared as output layers.\n",
    "        patch_size: integer, defines the size of the input (patch_size x patch_size).\n",
    "    Attributes:\n",
    "        loss_model: multi-output vgg architecture with <layers_to_extract> as output layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patch_size, layers_to_extract):\n",
    "        self.patch_size = patch_size\n",
    "        self.input_shape = (patch_size,) * 2 + (3,)\n",
    "        self.layers_to_extract = layers_to_extract\n",
    "        \n",
    "        if len(self.layers_to_extract) > 0:\n",
    "            self._cut_vgg()\n",
    "    \n",
    "    def _cut_vgg(self):\n",
    "        \"\"\"\n",
    "        Loads pre-trained VGG, declares as output the intermediate\n",
    "        layers selected by self.layers_to_extract.\n",
    "        \"\"\"\n",
    "        \n",
    "        vgg = VGG19(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "        vgg.trainable = False\n",
    "        outputs = [vgg.layers[i].output for i in self.layers_to_extract]\n",
    "        self.model = Model([vgg.input], outputs)\n",
    "feature_extraction = Cut_VGG19(128,[5,9])   \n",
    "feature_extraction.model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fadein(models, step, n_steps):\n",
    "    # calculate current alpha (linear from 0 to 1)\n",
    "    alpha = step / float(n_steps - 1)\n",
    "    # update the alpha for each model\n",
    "    for model in models:\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, WeightedSum):\n",
    "                K.set_value(layer.alpha, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self,pre_train = False, **kwargs):\n",
    "        self.input_dim = (256,256,3)\n",
    "        self.optimiser = 'rmsprop'\n",
    "        self.z_dim = 256\n",
    "        self.n_batch = [128,64,32,16,8,4] # batch_size\n",
    "        self.pre_train = pre_train\n",
    "        self.pre_fid = 500\n",
    "        ################ Encoder Model ########################\n",
    "        self.encoder_model = tf.keras.models.load_model(\"../VAE_model/model_VAE/encoder_model_256_2layer.h5\", \\\n",
    "                                                       custom_objects={\"Sampling\":Sampling})\n",
    "        self.decoder_model = tf.keras.models.load_model(\"../VAE_model/model_VAE/decoder_model_256_2layer.h5\", \\\n",
    "                                                       custom_objects={\"Sampling\":Sampling})\n",
    "#         ############## Conditional Layer #######################\n",
    "        self.conditional_input = (self.z_dim,)\n",
    "        self.conditional_initial_dense_layer_size = (4,4,32)\n",
    "        self.conditional_batch_norm_momentum =  None\n",
    "        self.conditional_activation = 'leaky_relu'\n",
    "        \n",
    "        ################ Generator Model #########################\n",
    "        self.generator_initial_dense_layer_size = (4,4,256)\n",
    "        self.generator_upsample = [1,1,1]\n",
    "        self.generator_conv_filters = [128,128,3]\n",
    "        self.generator_conv_kernel_size = [4,3,1]\n",
    "        self.generator_conv_strides = [2,2,2]\n",
    "        self.generator_batch_norm_momentum =  None\n",
    "        self.generator_activation = 'leaky_relu'\n",
    "        self.generator_dropout_rate = None\n",
    "        self.generator_learning_rate = 2e-3\n",
    "        self.gen_n_blocks = len(self.n_batch)\n",
    "        \n",
    "        ################ Discriminator Model ###########################\n",
    "        self.discriminator_input_shape = (4,4,3)\n",
    "        self.discriminator_conv_filters = [128,128,128]\n",
    "        self.discriminator_conv_kernel_size = [1,3,4]\n",
    "        self.discriminator_conv_strides = [1,1,1]\n",
    "        self.discriminator_batch_norm_momentum = None\n",
    "        self.discriminator_activation = 'leaky_relu'\n",
    "        self.discriminator_dropout_rate = None\n",
    "        self.discriminator_learning_rate = 2e-3\n",
    "        self.disc_n_blocks = len(self.n_batch)\n",
    "        ############################################        \n",
    "        self.n_layers_discriminator = len(self.discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(self.generator_conv_filters)\n",
    "        \n",
    "        ###############################################                               \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        self.epoch = 2010\n",
    "        ###############################################\n",
    "        if self.pre_train:\n",
    "            self.weight_init = None\n",
    "            self.grad_weight = 10\n",
    "            self.const = tf.keras.constraints.max_norm(1.0)\n",
    "            self.g_models = []\n",
    "            self.d_models = []\n",
    "            for i in range(0, len(self.n_batch)):\n",
    "                # scale dataset to appropriate size\n",
    "                output_shape = 4*np.power(2,i)\n",
    "                print(output_shape)\n",
    "                g_normal = tf.keras.models.load_model(\"./model_save/model_condproGAN/g_normal_\"+str(output_shape)+\".h5\", \\\n",
    "                                                      custom_objects={'PixelNormalization':PixelNormalization, \\\n",
    "                                                                     'MinibatchStdev': MinibatchStdev, \\\n",
    "                                                                     'WeightedSum':WeightedSum})\n",
    "                \n",
    "                g_fadein = tf.keras.models.load_model(\"./model_save/model_condproGAN/g_fadein_\"+str(output_shape)+\".h5\", \\\n",
    "                                                      custom_objects={'PixelNormalization':PixelNormalization, \\\n",
    "                                                                     'MinibatchStdev': MinibatchStdev, \\\n",
    "                                                                     'WeightedSum':WeightedSum})\n",
    "                \n",
    "                d_normal = tf.keras.models.load_model(\"./model_save/model_condproGAN/d_normal_\"+str(output_shape)+\".h5\", \\\n",
    "                                                      custom_objects={'AveragePooling2D':AveragePooling2D, \\\n",
    "                                                                     'MinibatchStdev': MinibatchStdev, \\\n",
    "                                                                     'RandomWeightedAverage':RandomWeightedAverage, \\\n",
    "                                                                     'WeightedSum':WeightedSum, \\\n",
    "                                                                     'wasserstein':self.wasserstein})\n",
    "                \n",
    "                d_fadein = tf.keras.models.load_model(\"./model_save/model_condproGAN/d_fadein_\"+str(output_shape)+\".h5\", \\\n",
    "                                                    custom_objects={'AveragePooling2D':AveragePooling2D, \\\n",
    "                                                                   'MinibatchStdev': MinibatchStdev, \\\n",
    "                                                                   'RandomWeightedAverage':RandomWeightedAverage, \\\n",
    "                                                                    'WeightedSum':WeightedSum, \\\n",
    "                                                                   'wasserstein':self.wasserstein})\n",
    "                \n",
    "                \n",
    "                self.g_models.append([g_normal, g_fadein])\n",
    "                self.d_models.append([d_normal,d_fadein])\n",
    "\n",
    "        else:\n",
    "            self.weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "            self.grad_weight = 10\n",
    "            self.const = tf.keras.constraints.max_norm(1.0)\n",
    "            self.g_models = self._build_generator()\n",
    "            self.d_models = self._build_discriminator()\n",
    "\n",
    "        self._build_adversarial()\n",
    "        LOG_DIR = \"./logs/cond_pro_gan.log\"\n",
    "        logging.basicConfig(filename=LOG_DIR,  \n",
    "                    level=logging.DEBUG,\n",
    "                    format=\"[%(asctime)s] [%(name)s] [%(message)s]\",\n",
    "                    filemode=\"a\")\n",
    "        logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "        logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "    ####################### Loss ###########################\n",
    "    def wasserstein(self, y_true, y_pred):\n",
    "        return -K.mean(y_true * y_pred)\n",
    "    \n",
    "    def get_perceptual_loss(self,y_true,y_pred):\n",
    "        content_feature = feature_extraction.model(y_true)\n",
    "        new_feature = feature_extraction.model(y_pred)\n",
    "        perceptual_loss = 0\n",
    "        weight = tf.constant([1/16,1/8], dtype = tf.float32)\n",
    "        for i in range(len(new_feature)):\n",
    "            perceptual_loss += weight[i]*K.mean(K.square(new_feature[i] - content_feature[i]))\n",
    "        l2_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true,y_pred))\n",
    "        total_loss = perceptual_loss + 100*l2_loss\n",
    "        return total_loss\n",
    "    \n",
    "    def dummy_loss_function(self,y_true, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    ################# Activation layer #####################                                                                \n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu':\n",
    "            layer = LeakyReLU(alpha = 0.2)\n",
    "        else:\n",
    "            layer = Activation(activation)\n",
    "        return layer\n",
    "    \n",
    "    \n",
    "    ####################################################################\n",
    "    #################### Build Generator Model #########################\n",
    "    ####################################################################\n",
    "    \n",
    "    def _build_generator(self):\n",
    "        ############  generator ###############\n",
    "        #with strategy.scope():\n",
    "        generator_input_layer = Input(shape=(self.z_dim,), name='generator_input')\n",
    "        x = generator_input_layer\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "\n",
    "        if self.generator_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)       \n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "\n",
    "        if self.generator_dropout_rate:\n",
    "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "\n",
    "        ###########################################################\n",
    "        for i in range(self.n_layers_generator): \n",
    "#             x = UpSampling2D()(x)\n",
    "            x = Conv2D(\n",
    "            filters = self.generator_conv_filters[i]\n",
    "            , kernel_size = (self.generator_conv_kernel_size[i],self.generator_conv_kernel_size[i])\n",
    "            , padding = 'same'\n",
    "            , kernel_initializer = self.weight_init\n",
    "            , kernel_constraint=self.const\n",
    "            )(x)\n",
    "\n",
    "            if i < self.n_layers_generator - 1:\n",
    "                x = PixelNormalization()(x)\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "                x = self.get_activation(self.generator_activation)(x)                \n",
    "        \n",
    "        generator_output = x\n",
    "        generator_model = Model(generator_input_layer, generator_output,name='Generator')\n",
    "        #generator_model.summary()\n",
    "        #################### Store Model #######################\n",
    "        model_list = list()\n",
    "        model_list.append([generator_model, generator_model])\n",
    "        # create submodels\n",
    "        for i in range(1, self.gen_n_blocks):\n",
    "            # get prior model without the fade-on\n",
    "            old_model = model_list[i - 1][0]\n",
    "\n",
    "            # create new model for next resolution\n",
    "            models = self.add_generator_block(old_model, i)\n",
    "#             models[0].summary()\n",
    "#             models[1].summary()\n",
    "            # store model\n",
    "            model_list.append(models)\n",
    "        return model_list\n",
    "    #################### Add Generator Block ################\n",
    "    def add_generator_block(self,old_model, model_indx):\n",
    "        # weight initialization\n",
    "        init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        # weight constraint\n",
    "        const = tf.keras.constraints.max_norm(1.0)\n",
    "        # get the end of the last block\n",
    "        block_end = old_model.layers[-2].output\n",
    "        # upsample, and define new block\n",
    "        #with strategy.scope():\n",
    "        upsampling = UpSampling2D()(block_end)\n",
    "        g = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(upsampling)\n",
    "        g = PixelNormalization()(g)\n",
    "        g = LeakyReLU(alpha=0.2)(g)\n",
    "        g = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "        g = PixelNormalization()(g)\n",
    "        g = LeakyReLU(alpha=0.2)(g)\n",
    "        # add new output layer\n",
    "        out_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "        # define model\n",
    "        model1 = Model(old_model.input, out_image, name = \"generator_normal_\"+str(model_indx))\n",
    "        # get the output layer from old model\n",
    "        out_old = old_model.layers[-1]\n",
    "        # connect the upsampling to the old output layer\n",
    "        out_image2 = out_old(upsampling)\n",
    "        # define new output image as the weighted sum of the old and new models\n",
    "        merged = WeightedSum()([out_image2, out_image])\n",
    "        # define model\n",
    "        model2 = Model(old_model.input, merged, name = \"generator_fadein_\"+str(model_indx))\n",
    "        return [model1, model2]\n",
    "        \n",
    "    ####################################################################\n",
    "    #################### Build Discriminator Model #####################\n",
    "    ####################################################################\n",
    "    def _build_discriminator(self):\n",
    "        #with strategy.scope():\n",
    "        discriminator_input = Input(shape=self.discriminator_input_shape, name='discriminator_input')\n",
    "        ############## Conditional Layer ####################\n",
    "        conditional_input = Input(shape= self.conditional_input, name='conditional_input')\n",
    "        x_cond = conditional_input\n",
    "        x_cond = Dense(np.prod(self.conditional_initial_dense_layer_size), kernel_initializer = self.weight_init)(x_cond)\n",
    "        if self.conditional_batch_norm_momentum:\n",
    "            x_cond = BatchNormalization(momentum = self.conditional_batch_norm_momentum)(x_cond)       \n",
    "        x_cond = self.get_activation(self.conditional_activation)(x_cond)\n",
    "        x_cond = Reshape(self.conditional_initial_dense_layer_size)(x_cond)\n",
    "       \n",
    "        x= Concatenate()([discriminator_input,x_cond])\n",
    "        #x = discriminator_input\n",
    "\n",
    "        for i in range(self.n_layers_discriminator):\n",
    "            x = Conv2D(\n",
    "                filters = self.discriminator_conv_filters[i]\n",
    "                , kernel_size = (self.discriminator_conv_kernel_size[i],self.discriminator_conv_kernel_size[i])\n",
    "                , strides = self.discriminator_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , kernel_initializer = self.weight_init\n",
    "                , kernel_constraint= self.const\n",
    "                )(x)\n",
    "\n",
    "            if self.discriminator_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n",
    "            x = self.get_activation(self.discriminator_activation)(x)\n",
    "            if self.discriminator_dropout_rate:\n",
    "                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n",
    "            ############### Concatenate ##########################\n",
    "#             if i == 0:\n",
    "#                 x = Concatenate()([x,x_cond])\n",
    "            if i <self.n_layers_discriminator-1:\n",
    "                x = MinibatchStdev()(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        discriminator_output = Dense(1, activation=None\n",
    "        , kernel_initializer = self.weight_init\n",
    "        )(x)\n",
    "\n",
    "        discriminator_model = Model([discriminator_input,conditional_input], discriminator_output,name=\"Discriminator\")\n",
    "\n",
    "        model_list = list()\n",
    "\n",
    "        model_list.append([discriminator_model, discriminator_model])\n",
    "        # create submodels\n",
    "        for i in range(1, self.disc_n_blocks):\n",
    "            # get prior model without the fade-on\n",
    "            old_model = model_list[i - 1][0]\n",
    "\n",
    "            # create new model for next resolution\n",
    "            models = self.add_discriminator_block(old_model, i)\n",
    "            model_list.append(models)\n",
    "        return model_list\n",
    "    #################### Add Discriminator Block ################\n",
    "    def add_discriminator_block(self,old_model,model_indx, n_input_layers=8):\n",
    "        # weight initialization\n",
    "        init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        # weight constraint\n",
    "        const = tf.keras.constraints.max_norm(1.0)\n",
    "        # get shape of existing model\n",
    "        #with strategy.scope():\n",
    "        in_shape = list(old_model.input[0].shape)\n",
    "        \n",
    "        # define new input shape as double the size\n",
    "        input_shape = (in_shape[-2]*2, in_shape[-2]*2, in_shape[-1])\n",
    "        # define new conditional layer\n",
    "        conditional_input = Input(shape= self.conditional_input)\n",
    "        x_cond = conditional_input\n",
    "        cond_initial_dense = (self.conditional_initial_dense_layer_size[0]*np.power(2,model_indx), \\\n",
    "                              self.conditional_initial_dense_layer_size[1]*np.power(2,model_indx), \\\n",
    "                              self.conditional_initial_dense_layer_size[2])\n",
    "    \n",
    "        x_cond = Dense(np.prod(cond_initial_dense), kernel_initializer = init)(x_cond)\n",
    "        if self.conditional_batch_norm_momentum:\n",
    "            x_cond = BatchNormalization(momentum = self.conditional_batch_norm_momentum)(x_cond)       \n",
    "        x_cond = self.get_activation(self.conditional_activation)(x_cond)\n",
    "        x_cond = Reshape(cond_initial_dense)(x_cond)\n",
    "        \n",
    "        in_image = Input(shape=input_shape)\n",
    "        ############ Concatenate\n",
    "        d = Concatenate()([in_image,x_cond])\n",
    "        \n",
    "        # define new input processing layer\n",
    "        d = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        # define new block\n",
    "        d = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        d = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        d = AveragePooling2D()(d)\n",
    "       \n",
    "        #d = RoiPoolingConv(2)(d)\n",
    "        block_new = d\n",
    "        # skip the input, 1x1 and activation for the old model\n",
    "        for i in range(n_input_layers, len(old_model.layers)):\n",
    "            d = old_model.layers[i](d)\n",
    "        # define straight-through model\n",
    "        \n",
    "        model1 = Model([in_image,conditional_input], d, name = \"discriminator_normal_\"+str(model_indx))\n",
    "        # compile model\n",
    "        model1.compile(loss=self.wasserstein, optimizer=RMSprop(lr=self.discriminator_learning_rate))\n",
    "        ###################################################################\n",
    "\n",
    "        d = Concatenate()([in_image,x_cond])\n",
    "        downsample = AveragePooling2D()(d)\n",
    "        # connect old input processing to downsampled new input\n",
    "\n",
    "        block_old = old_model.layers[6](downsample)\n",
    "        block_old = old_model.layers[7](block_old)\n",
    "\n",
    "        # fade in output of old model input layer with new input\n",
    "        d = WeightedSum()([block_old, block_new])\n",
    "\n",
    "        # skip the input, 1x1 and activation for the old model\n",
    "        for i in range(n_input_layers, len(old_model.layers)):\n",
    "            d = old_model.layers[i](d)\n",
    "        # define straight-through model\n",
    "        model2 = Model([in_image,conditional_input], d, name= \"discriminator_fadein_\"+str(model_indx))\n",
    "        return [model1, model2]\n",
    "   \n",
    "\n",
    "    #################### Optimize #########################\n",
    "\n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "\n",
    "\n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val\n",
    "        for l in m.layers:\n",
    "            l.trainable = val\n",
    "            \n",
    "    def update_fadein(self,models, step, n_steps):\n",
    "        # calculate current alpha (linear from 0 to 1)\n",
    "        alpha = step / float(n_steps - 1)\n",
    "        # update the alpha for each model\n",
    "        for model in models:\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, WeightedSum):\n",
    "                    K.set_value(layer.alpha, alpha)\n",
    "    ####################################################################\n",
    "    #################### Build Adversarial Model #########################\n",
    "    ####################################################################\n",
    "    def build_discriminator_model(self,discriminator, batch_size):\n",
    "        #with strategy.scope():\n",
    "        real_input = discriminator.input[0]\n",
    "        real_conditional_input = discriminator.input[1]\n",
    "        \n",
    "        shape = real_input.shape[1:]\n",
    "        shape_conditional = real_conditional_input.shape[1]\n",
    "        fake_input = Input(shape = shape)\n",
    "        fake_conditional_input = Input(shape=(shape_conditional,))\n",
    "        \n",
    "        discriminator_output_from_generator = discriminator([fake_input,fake_conditional_input])\n",
    "        discriminator_output_from_real_samples = discriminator([real_input,real_conditional_input])\n",
    "        \n",
    "        averaged_samples = RandomWeightedAverage(batch_size = batch_size)([[real_input,real_conditional_input], \\\n",
    "                                                [fake_input,fake_conditional_input]])\n",
    "        \n",
    "        validity_interpolated = discriminator([averaged_samples,real_conditional_input])\n",
    "        \n",
    "        gp = GradientPenalty()([validity_interpolated, averaged_samples])\n",
    "#         partial_gp_loss = partial(gradient_penalty,\n",
    "#                               discriminator = discriminator,\n",
    "#                               gradient_penalty_weight=10)\n",
    "\n",
    "#         partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "\n",
    "\n",
    "\n",
    "        discriminator_model = Model(inputs = [[real_input,real_conditional_input], \\\n",
    "                                              [fake_input,fake_conditional_input]], \\\n",
    "                      outputs = [discriminator_output_from_real_samples,discriminator_output_from_generator,gp])\n",
    "\n",
    "        discriminator_model.compile(optimizer=Adam(self.discriminator_learning_rate, beta_1=0.5, beta_2=0.9),\n",
    "                                loss=[self.wasserstein,\n",
    "                                      self.wasserstein,\n",
    "                                        \"mse\"])\n",
    "        #discriminator_model.summary()\n",
    "        return discriminator_model\n",
    "    \n",
    "    def define_composite(self,discriminators, generators):\n",
    "        model_list = []\n",
    "        #with strategy.scope():\n",
    "        for i in range(len(discriminators)):\n",
    "            if i != len(discriminators)-1 :\n",
    "                discriminators[i][0].trainable = False\n",
    "                input_dis_cond = generators[i][0].input\n",
    "                input_dis = generators[i][0].output\n",
    "                output = discriminators[i][0]([input_dis,input_dis_cond])\n",
    "                model1 = Model(generators[i][0].input, output)\n",
    "                model1.compile(loss = self.wasserstein, optimizer = RMSprop(lr=self.generator_learning_rate))\n",
    "                discriminators[i][0].trainable = True\n",
    "\n",
    "                discriminators[i][1].trainable = False\n",
    "                input_dis_cond = generators[i][1].input\n",
    "                input_dis = generators[i][1].output\n",
    "                output = discriminators[i][1]([input_dis,input_dis_cond])\n",
    "                model2 = Model(generators[i][1].input, output)\n",
    "                model2.compile(loss = self.wasserstein, optimizer = RMSprop(lr=self.generator_learning_rate))\n",
    "                discriminators[i][1].trainable = True\n",
    "\n",
    "            else:\n",
    "                discriminators[i][0].trainable = False\n",
    "                input_dis_cond = generators[i][0].input\n",
    "                input_dis = generators[i][0].output\n",
    "                output = discriminators[i][0]([input_dis,input_dis_cond])\n",
    "                model1 = Model(generators[i][0].input, [output,input_dis])\n",
    "                model1.compile(loss = [self.wasserstein, self.get_perceptual_loss], optimizer = RMSprop(lr=self.generator_learning_rate))\n",
    "                discriminators[i][0].trainable = True\n",
    "\n",
    "                discriminators[i][1].trainable = False\n",
    "                input_dis_cond = generators[i][1].input\n",
    "                input_dis = generators[i][1].output\n",
    "                output = discriminators[i][1]([input_dis,input_dis_cond])\n",
    "                model2 = Model(generators[i][1].input, [output,input_dis])\n",
    "                model2.compile(loss =  [self.wasserstein, self.get_perceptual_loss], optimizer = RMSprop(lr=self.generator_learning_rate))\n",
    "                discriminators[i][1].trainable = True\n",
    "                \n",
    "            model_list.append([model1, model2])\n",
    "\n",
    "        return model_list\n",
    "    ####################################################################\n",
    "    #################### Build Adversarial Model #########################\n",
    "    ####################################################################\n",
    "    def _build_adversarial(self):\n",
    "        self.disc_models = []\n",
    "        ## build discriminator \n",
    "        for i in range(len(self.d_models)):\n",
    "            self.disc_models.append([self.build_discriminator_model(self.d_models[i][0],int(self.n_batch[i])),self.build_discriminator_model(self.d_models[i][1], int(self.n_batch[i]))])\n",
    "\n",
    "        ## build gan model\n",
    "        self.gan_models = self.define_composite(self.d_models, self.g_models)    \n",
    "    \n",
    "    def train_discriminator(self,g_model,d_model, x_train,Y_train, batch_size):\n",
    "\n",
    "        valid = np.ones((batch_size,1))\n",
    "        fake = -np.ones((batch_size,1))\n",
    "        dummy = np.zeros((int(batch_size), 1), dtype=np.float32) # Dummy gt for gradient penalty\n",
    "  \n",
    "        images_input = x_train\n",
    "        true_imgs = Y_train\n",
    "        \n",
    "        latent_code = self.encoder_model.predict(images_input)[2]\n",
    "        gen_imgs = g_model.predict(latent_code)\n",
    "        \n",
    "        d_loss = d_model.train_on_batch([[true_imgs,latent_code],[gen_imgs,latent_code]],[valid, fake, dummy])\n",
    "   \n",
    "        return d_loss\n",
    "\n",
    "    def train_generator(self,gan_model,x_train,Y_train, batch_size):\n",
    "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "        true_images = Y_train\n",
    "        input_images = x_train\n",
    "        \n",
    "        latent_code = self.encoder_model.predict(input_images)[2]\n",
    "        #latencode = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        \n",
    "        if type(gan_model.output)==list:\n",
    "            gans_loss = gan_model.train_on_batch(latent_code, [valid,Y_train])\n",
    "        else:\n",
    "            gans_loss = gan_model.train_on_batch(latent_code, valid)\n",
    "        return gans_loss\n",
    "    \n",
    "    ########################################################\n",
    "    ########################################################\n",
    "    def scaled_data(self,data,input_shape):\n",
    "        images_arr = []\n",
    "        for img in data:\n",
    "            #print(img.shape)\n",
    "            img = array_to_img(img)\n",
    "            resized_img = img.resize(size=input_shape[:-1])\n",
    "            images_arr.append(img_to_array(resized_img))\n",
    "        data = np.asarray(images_arr)\n",
    "        data = (data - 127.5)/127.5\n",
    "        data = data.astype('float32')\n",
    "        return data\n",
    "    \n",
    "    def shuffle_data_batch(self,array_X,array_Y,batch_size):\n",
    "        indices = np.arange(array_X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        array_X = array_X[indices]\n",
    "        array_Y = array_Y[indices]\n",
    "        return array_X,array_Y\n",
    "    \n",
    "    def shuffle_data_batch_with_latent(self,array_latent,array_X,array_Y,batch_size):\n",
    "        indices = np.arange(array_X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        array_latent = array_latent[indices]\n",
    "        array_X = array_X[indices]\n",
    "        array_Y = array_Y[indices]\n",
    "        return array_latent,array_X,array_Y\n",
    "    \n",
    "    def split_into_chunks(self,l, n):\n",
    "        for i in range(0, l.shape[0], n):\n",
    "            yield l[i:i + n]  \n",
    "    \n",
    "    def scale_all_shape(self,data):\n",
    "        init_shape = self.discriminator_input_shape[0]\n",
    "        num_scale = len(self.n_batch)\n",
    "        data_all_shape = []\n",
    "        for i in range(num_scale):\n",
    "            shape = (init_shape*np.power(2,i),init_shape*np.power(2,i),3)\n",
    "            data_scaled = self.scaled_data(data,shape)\n",
    "            data_all_shape.append(data_scaled)\n",
    "        return data_all_shape\n",
    "    \n",
    "    def train_epochs(self,g_model, d_model, gan_model, x_train, Y_train,n_critic=5,fadein=False):\n",
    "        \n",
    "        for i in range(self.n_steps):\n",
    "            if fadein:\n",
    "                update_fadein([g_model, d_model, gan_model], i, self.n_steps)\n",
    "            x = next(x_train)\n",
    "            Y = next(Y_train)\n",
    "            for _ in range(n_critic):\n",
    "                d_loss = self.train_discriminator(g_model,d_model,x, Y, self.batch_size)\n",
    "            g_loss = self.train_generator(gan_model,x ,Y , self.batch_size)\n",
    "            # Plot the progress\n",
    "            self.d_losses.append(d_loss)\n",
    "            self.g_losses.append(g_loss)\n",
    "            return d_loss,g_loss\n",
    "    def train(self,x_train,Y_train,x_val,Y_val, batch_size, epochs, run_folder, print_every_n_batches = 10, n_critic=2):\n",
    "        self.data_fid = ((Y_train*127.5)+127.5).astype('float32')\n",
    "        self.data_fid = scale_images(self.data_fid, (299,299,3))\n",
    "        Y_train_reshape = self.scale_all_shape(Y_train)\n",
    "        \n",
    "        for epoch in range(self.epoch, self.epoch + epochs):\n",
    "            for i in range(0, len(self.g_models)):\n",
    "                self.batch_size = self.n_batch[i]\n",
    "                x_train_gan,Y_train_gan = self.shuffle_data_batch(x_train,Y_train_reshape[i],self.batch_size)\n",
    "                self.n_steps = int(x_train.shape[0]/self.batch_size)-1\n",
    "                # scale dataset to appropriate size\n",
    "                [g_normal, g_fadein] = self.g_models[i]\n",
    "                [d_normal, d_fadein] = self.disc_models[i]\n",
    "                [gan_normal, gan_fadein] = self.gan_models[i]\n",
    "                gen_shape = g_normal.output_shape\n",
    "             \n",
    "                Y_train_gan = self.split_into_chunks(Y_train_gan,self.batch_size)\n",
    "                x_train_gan = self.split_into_chunks(x_train_gan,self.batch_size)\n",
    "                \n",
    "                # Train with fadein\n",
    "                d_loss_fadein, g_loss_fadein = self.train_epochs(g_fadein, d_fadein, gan_fadein,x_train_gan, Y_train_gan,n_critic, True)\n",
    "                # Train without fadein\n",
    "                d_loss, g_loss = self.train_epochs(g_normal, d_normal, gan_normal, x_train_gan, Y_train_gan,n_critic, False)\n",
    "                \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % print_every_n_batches == 0:\n",
    "                # calculator FID \n",
    "                fid_fadein = self.sample_images(x_val,Y_val,run_folder,self.g_models[-1][1], fadein = True)\n",
    "                fid_normal = self.sample_images(x_val,Y_val,run_folder,self.g_models[-1][0], fadein = False)\n",
    "                logging.info(json.dumps({'epoch':epoch,'d_fadein_loss':d_loss_fadein,'g_fadein_loss':g_loss_fadein,'fid_fadein':fid_fadein, \\\n",
    "                                                        'd_normal_loss':d_loss,'g_normal_loss':g_loss,'fid_normal':fid_normal, \\\n",
    "                                                         'fid_min':self.pre_fid})) \n",
    "                print (epoch,\"=======Fadein====\",d_loss_fadein, g_loss_fadein,fid_fadein,fid_normal)  \n",
    "                if fid_normal < self.pre_fid:\n",
    "                    self.pre_fid = fid_normal\n",
    "                    for i in range(0, len(self.g_models)):\n",
    "                        # scale dataset to appropriate size\n",
    "                        [g_normal, g_fadein] = self.g_models[i]\n",
    "                        [d_normal, d_fadein] = self.d_models[i]\n",
    "                        #[gan_normal, gan_fadein] = GAN.gan_models[i]\n",
    "                        output_shape = g_normal.output_shape[1]\n",
    "                        g_normal.save(\"./model_save/model_condproGAN/g_normal_\"+str(output_shape)+\".h5\")\n",
    "                        g_fadein.save(\"./model_save/model_condproGAN/g_fadein_\"+str(output_shape)+\".h5\")\n",
    "                        d_normal.save(\"./model_save/model_condproGAN/d_normal_\"+str(output_shape)+\".h5\")\n",
    "                        d_fadein.save(\"./model_save/model_condproGAN/d_fadein_\"+str(output_shape)+\".h5\")\n",
    "            self.epoch+=1\n",
    "\n",
    "    def sample_images(self,x_val,Y_val, run_folder, g_model, fadein = True):\n",
    "        fid = self.pre_fid\n",
    "        if self.epoch % 40 ==0:\n",
    "            # Test\n",
    "            r, c = 8, 4\n",
    "            output_shape = g_model.output_shape[1:]\n",
    "            y_true = Y_val\n",
    "            input_model = x_val\n",
    "\n",
    "            latent_code = self.encoder_model.predict(input_model)[2]\n",
    "            #gen_finger = self.decoder_model.predict(latent_code)\n",
    "            gen_imgs = g_model.predict(latent_code)\n",
    "            ########## FID ##############\n",
    "\n",
    "            \n",
    "            #############################\n",
    "            indx = np.random.choice(y_true.shape[0], int(0.5*c*r) ,replace=False)\n",
    "\n",
    "            face_real = 0.5*(y_true[indx]+1)\n",
    "            face_real = np.clip(face_real, 0, 1)\n",
    "            finger_real = input_model[indx]\n",
    "            #face_real = face_real[:,:,:,[2,1,0]]\n",
    "\n",
    "            gen_imgs = 0.5 * (gen_imgs[indx] + 1)\n",
    "            gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "            #gen_imgs = gen_imgs[:,:,:,[2,1,0]]\n",
    "            #gen_finger = 0.5 * (gen_finger[indx] + 1)\n",
    "            #gen_finger = np.clip(gen_finger, 0, 1)\n",
    "\n",
    "            fig, axs = plt.subplots(r, c, figsize=(15,30))\n",
    "            #fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss))\n",
    "            cnt = 0\n",
    "            for i in range(r):\n",
    "                for j in range(int(0.5*c)):\n",
    "                    axs[i,2*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "                    axs[i,2*j].axis('off')\n",
    "                    axs[i,2*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "                    axs[i,2*j+1].axis('off')\n",
    "    #                 axs[i,4*j+2].imshow(np.squeeze(gen_finger[cnt, :,:,:]),cmap='gray')\n",
    "    #                 axs[i,4*j+2].axis('off')\n",
    "    #                 axs[i,4*j+3].imshow(np.squeeze(finger_real[cnt, :,:,:]),cmap='gray')\n",
    "    #                 axs[i,4*j+3].axis('off')\n",
    "                    cnt += 1\n",
    "        \n",
    "            #fid = calculate_fid(model_inceptionv3,gen_imgs,self.data_fid)\n",
    "            if fadein:\n",
    "                fig.savefig(os.path.join(run_folder, \"images_latent/sample_fadein_%d_%d.png\" % (output_shape[0],self.epoch)))\n",
    "            else:\n",
    "                fig.savefig(os.path.join(run_folder, \"images_latent/sample_normal_%d_%d.png\" % (output_shape[0],self.epoch)))\n",
    "        plt.close()\n",
    "        return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "4\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "8\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "16\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "32\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "64\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "128\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "GAN = WGANGP(pre_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_face_fingerprint(data_face,data_finger, num_augment_percent=0.8):\n",
    "    num_data = data_face.shape[0]\n",
    "    num_data_augment = int(data_face.shape[0]*num_augment_percent)\n",
    "    index_data = np.random.choice(data_face.shape[0],num_data_augment,replace=False)\n",
    "    \n",
    "    data_aug_face = []\n",
    "    data_aug_finger = []\n",
    "    for i in index_data:\n",
    "        data_aug_face.append(data_face[i,:,:,:])\n",
    "        data_aug_finger.append(data_finger[i,:,:,:])\n",
    "    data_aug_face = np.asarray(data_aug_face)    \n",
    "    data_aug_finger = np.asarray(data_aug_finger) \n",
    "\n",
    "    # create image data augmentation generator\n",
    "    datagen_face = ImageDataGenerator(horizontal_flip=True)\n",
    "    datagen_face.fit(data_aug_face)\n",
    "    \n",
    "    datagen_finger = ImageDataGenerator(horizontal_flip=False,height_shift_range=0.1,width_shift_range=0.1,shear_range=0.5,rotation_range=20)\n",
    "    datagen_finger.fit(data_aug_finger)\n",
    "    \n",
    "    it_face = datagen_face.flow(data_aug_face,batch_size=num_data_augment,shuffle=False)\n",
    "    it_finger = datagen_finger.flow(data_aug_finger,batch_size=num_data_augment,shuffle=False)\n",
    "    \n",
    "    results_face = np.concatenate([data_face,it_face.next()],axis=0)\n",
    "    results_finger = np.concatenate([data_finger,it_finger.next()],axis=0)\n",
    "    #np.random.shuffle(results)\n",
    "    return results_face,results_finger\n",
    "\n",
    "def augment_data_only_fingerprint(data_finger, num_augment_percent = 0.5):\n",
    "    \n",
    "    num_data = data_finger.shape[0]\n",
    "    num_data_augment = int(data_finger.shape[0]*num_augment_percent)\n",
    "    index_batch = np.arange(0,num_data_augment)\n",
    "    index_data = np.random.choice(num_data,num_data_augment,replace=False)\n",
    "    ###############################\n",
    "    data_aug_finger = []\n",
    "    for i in index_data:\n",
    "        data_aug_finger.append(data_finger[i,:,:,:])   \n",
    "    data_aug_finger = np.asarray(data_aug_finger) \n",
    "    ################################\n",
    "    datagen_finger = ImageDataGenerator(horizontal_flip=False,height_shift_range=0.2,width_shift_range=0.2,shear_range=0.1,rotation_range=20)\n",
    "    datagen_finger.fit(data_aug_finger)\n",
    "    ###############################\n",
    "    it_finger = datagen_finger.flow(data_aug_finger,batch_size=num_data_augment,shuffle=False)\n",
    "    finger_augmented = it_finger.next()\n",
    "    ################################\n",
    "    result_finger = data_finger\n",
    "    for index in zip(index_data,index_batch):\n",
    "        result_finger[index[0],:,:,:]=finger_augmented[index[1],:,:,:]\n",
    "    return result_finger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data train with latent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_train/data_get_latent/'\n",
    "with open(os.path.join(DATA_DIR,\"data_face_2856_train.pkl\"), \"rb\") as input_file:\n",
    "    data_train_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_fingerprint_2856_train.pkl\"), \"rb\") as input_file:\n",
    "    data_train_finger = pickle.load(input_file)\n",
    "\n",
    "    \n",
    "with open(os.path.join(DATA_DIR,\"data_face_2856_val.pkl\"), \"rb\") as input_file:\n",
    "    data_val_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_fingerprint_2856_val.pkl\"), \"rb\") as input_file:\n",
    "    data_val_finger = pickle.load(input_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_face, data_train_finger = augment_data_face_fingerprint(data_train_face,data_train_finger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7711, 256, 256, 3),\n",
       " (7711, 256, 256, 1),\n",
       " (715, 256, 256, 3),\n",
       " (715, 256, 256, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_face.shape,data_train_finger.shape,data_val_face.shape,data_val_finger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_finger = ((data_train_finger)/np.max(data_train_finger))\n",
    "data_val_finger = ((data_val_finger)/np.max(data_val_finger))\n",
    "data_train_finger = np.where(data_train_finger > .5, 1.0, 0.0).astype('float32')\n",
    "data_val_finger = np.where(data_val_finger > .5, 1.0, 0.0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(data_val_face),np.max(data_val_face), data_val_face.dtype)\n",
    "plt.imshow(0.5*(data_val_face[150]+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_face = ((data_train_face-127.5)/127.5).astype('float32')\n",
    "# data_val_face = ((data_val_face-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_face = data_train_face\n",
    "val_face = data_val_face\n",
    "train_finger = data_train_finger\n",
    "val_finger = data_val_finger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 =======Fadein==== [-228.10498046875, -7037.68310546875, 6711.10986328125, 98.46807861328125] [-14060.98828125, -14073.84375, 12.85594654083252] 500 500\n",
      "2020 =======Fadein==== [-42.79930114746094, 130.9100341796875, -186.141357421875, 12.432028770446777] [-644.5880126953125, -658.91650390625, 14.328479766845703] 500 500\n",
      "2030 =======Fadein==== [-27.35528564453125, 209.2581787109375, -239.53314208984375, 2.919672966003418] [-1187.9765625, -1206.44873046875, 18.47211456298828] 500 500\n",
      "2040 =======Fadein==== [-5.182373046875, 3495.3154296875, -3503.159912109375, 2.662209987640381] [3043.60400390625, 3025.91015625, 17.693864822387695] 500 500\n",
      "2050 =======Fadein==== [-101.13134765625, -7203.13720703125, 7065.046875, 36.95896911621094] [-8452.2080078125, -8468.2587890625, 16.050437927246094] 500 500\n",
      "2060 =======Fadein==== [-56.7236328125, 8523.451171875, -8583.9638671875, 3.7888693809509277] [16007.595703125, 15991.232421875, 16.363006591796875] 500 500\n",
      "2070 =======Fadein==== [-282.1181640625, -13694.7255859375, 13169.8876953125, 242.71994018554688] [-17668.4140625, -17682.66796875, 14.25349235534668] 500 500\n",
      "2080 =======Fadein==== [-2.6429519653320312, -122.50697326660156, 119.40789794921875, 0.45612508058547974] [-50.34720230102539, -72.99296569824219, 22.645763397216797] 500 500\n",
      "2090 =======Fadein==== [1441.8828125, -47863.6171875, 48848.65625, 456.8443603515625] [1406.3358154296875, 1385.988525390625, 20.347261428833008] 500 500\n",
      "2100 =======Fadein==== [-35.0968017578125, -206.44606018066406, 170.64447021484375, 0.7047816514968872] [-1387.806640625, -1411.4317626953125, 23.62515640258789] 500 500\n",
      "2110 =======Fadein==== [-30.661285400390625, -388.09600830078125, 357.3086242675781, 0.1260937750339508] [-674.5673828125, -697.6031494140625, 23.035785675048828] 500 500\n",
      "2120 =======Fadein==== [-10.750244140625, -1110.0458984375, 1099.2760009765625, 0.01966656558215618] [-1106.032470703125, -1122.276611328125, 16.24420166015625] 500 500\n",
      "2130 =======Fadein==== [8316.28125, -403361.0, 398541.1875, 13136.1015625] [472.2279052734375, 444.0277404785156, 28.200172424316406] 500 500\n",
      "2140 =======Fadein==== [-28.8392333984375, -1808.594482421875, 1771.4124755859375, 8.342724800109863] [-1236.6448974609375, -1255.5517578125, 18.906841278076172] 500 500\n",
      "2150 =======Fadein==== [-30.7216796875, -9212.396484375, 9175.2392578125, 6.435301780700684] [-10742.6748046875, -10760.1474609375, 17.472732543945312] 500 500\n",
      "2160 =======Fadein==== [-46.78125, -2231.2548828125, 2180.3779296875, 4.095791816711426] [-3247.295166015625, -3266.3408203125, 19.04572868347168] 500 500\n",
      "2170 =======Fadein==== [-82.892578125, -19078.404296875, 18986.1328125, 9.37898063659668] [-28990.359375, -29010.376953125, 20.01770782470703] 500 500\n",
      "2180 =======Fadein==== [-34.45703125, -40743.01953125, 40684.8828125, 23.681198120117188] [-62858.43359375, -62874.7578125, 16.324440002441406] 500 500\n",
      "2190 =======Fadein==== [-15.80615234375, -4395.77099609375, 4374.123046875, 5.841733455657959] [-4008.33642578125, -4026.54248046875, 18.20612144470215] 500 500\n",
      "2200 =======Fadein==== [-34.376953125, -4205.86328125, 4160.0087890625, 11.47762680053711] [-1050.9466552734375, -1066.3201904296875, 15.373516082763672] 500 500\n",
      "2210 =======Fadein==== [-225.38671875, -37983.5078125, 37581.96875, 176.15411376953125] [1345.4407958984375, 1325.0775146484375, 20.363296508789062] 500 500\n",
      "2220 =======Fadein==== [-152.1826171875, -9988.7177734375, 9795.689453125, 40.845359802246094] [-29862.140625, -29880.3125, 18.171995162963867] 500 500\n",
      "2230 =======Fadein==== [14.763671875, 8456.25390625, -8441.92578125, 0.43559592962265015] [4370.02294921875, 4349.1005859375, 20.92227554321289] 500 500\n",
      "2240 =======Fadein==== [-21.66015625, 21314.78125, -21345.916015625, 9.473930358886719] [27246.349609375, 27228.537109375, 17.813392639160156] 500 500\n",
      "2250 =======Fadein==== [-23.05145263671875, 929.8045654296875, -956.8065795898438, 3.950545310974121] [1838.2562255859375, 1821.922607421875, 16.333646774291992] 500 500\n",
      "2260 =======Fadein==== [-52.5078125, 14474.466796875, -14540.095703125, 13.120911598205566] [-85454.2578125, -85469.59375, 15.339668273925781] 500 500\n",
      "2270 =======Fadein==== [-123.3671875, -7926.88330078125, 7766.11328125, 37.40260696411133] [12110.873046875, 12086.482421875, 24.390914916992188] 500 500\n",
      "2280 =======Fadein==== [-23.4921875, 40154.0390625, -40186.43359375, 8.900680541992188] [59203.99609375, 59188.55859375, 15.436502456665039] 500 500\n",
      "2290 =======Fadein==== [-41.1689453125, 10412.80078125, -10455.8388671875, 1.8693385124206543] [18788.32421875, 18772.26171875, 16.062543869018555] 500 500\n",
      "2300 =======Fadein==== [-49.65625, 21071.138671875, -21216.56640625, 95.77206420898438] [16360.537109375, 16347.13671875, 13.400493621826172] 500 500\n",
      "2310 =======Fadein==== [-32.9853515625, 9649.1318359375, -9684.765625, 2.6484200954437256] [2301.18505859375, 2284.60888671875, 16.576248168945312] 500 500\n",
      "2320 =======Fadein==== [-49.122314453125, 1476.2142333984375, -1531.016357421875, 5.679807662963867] [-8966.92578125, -8982.265625, 15.340089797973633] 500 500\n",
      "2330 =======Fadein==== [-0.6436996459960938, -46.3203125, 30.30810546875, 15.368507385253906] [-29297.29296875, -29315.947265625, 18.654502868652344] 500 500\n",
      "2340 =======Fadein==== [-23.67822265625, 4178.275390625, -4203.21923828125, 1.2654075622558594] [7729.90185546875, 7712.505859375, 17.396198272705078] 500 500\n",
      "2350 =======Fadein==== [-94.234375, -21257.0859375, 21140.09765625, 22.754283905029297] [-42974.43359375, -42996.90625, 22.47136688232422] 500 500\n",
      "2360 =======Fadein==== [45.095703125, 19282.203125, -19238.671875, 1.5640535354614258] [14969.0439453125, 14952.3046875, 16.73890495300293] 500 500\n",
      "2370 =======Fadein==== [30.92578125, 57560.3359375, -57531.296875, 1.8877456188201904] [54216.09375, 54195.78515625, 20.30735206604004] 500 500\n",
      "2380 =======Fadein==== [-5.560546875, 5598.93359375, -5604.66455078125, 0.1703576296567917] [14621.8994140625, 14605.22265625, 16.676774978637695] 500 500\n",
      "2390 =======Fadein==== [-6.6640625, -28952.673828125, 28936.26171875, 9.747213363647461] [-27745.234375, -27761.8984375, 16.664684295654297] 500 500\n",
      "2400 =======Fadein==== [-5.7060546875, 11264.265625, -11270.416015625, 0.44433489441871643] [56569.71875, 56550.953125, 18.766231536865234] 500 500\n",
      "2410 =======Fadein==== [-6.16455078125, 2938.631591796875, -2944.93115234375, 0.13495808839797974] [-2249.14599609375, -2268.552734375, 19.406848907470703] 500 500\n",
      "2420 =======Fadein==== [3.82421875, 10473.04296875, -10469.2265625, 0.008267438039183617] [16132.9072265625, 16114.1728515625, 18.733951568603516] 500 500\n",
      "2430 =======Fadein==== [50.109375, 18221.90625, -18174.025390625, 2.228339195251465] [14613.0458984375, 14584.5263671875, 28.519168853759766] 500 500\n",
      "2440 =======Fadein==== [3854.09375, -394064.8125, 396114.15625, 1804.753173828125] [15032.6328125, 15006.5302734375, 26.102069854736328] 500 500\n",
      "2450 =======Fadein==== [-13.3828125, -3667.46044921875, 3654.070556640625, 0.007172406185418367] [4745.71630859375, 4729.5908203125, 16.125442504882812] 500 500\n",
      "2460 =======Fadein==== [-1.6641845703125, -760.82177734375, 758.8133544921875, 0.34424179792404175] [-1902.5924072265625, -1919.883056640625, 17.290645599365234] 500 500\n",
      "2470 =======Fadein==== [-54.216796875, -8985.2890625, 8930.5615234375, 0.510347306728363] [-14417.208984375, -14443.046875, 25.837541580200195] 500 500\n",
      "2480 =======Fadein==== [-37.517578125, -17262.279296875, 17224.0546875, 0.7066560983657837] [-30892.283203125, -30916.1328125, 23.850187301635742] 500 500\n",
      "2490 =======Fadein==== [39.00390625, -61786.04296875, 61786.90625, 38.141807556152344] [-30921.82421875, -30941.53515625, 19.710926055908203] 500 500\n",
      "2500 =======Fadein==== [-11.99609375, -35171.60546875, 35159.46875, 0.1407274603843689] [-73426.8671875, -73441.96875, 15.10313606262207] 500 500\n",
      "2510 =======Fadein==== [3.443359375, -10016.9111328125, 10020.2841796875, 0.07037050276994705] [-8998.830078125, -9017.62890625, 18.798826217651367] 500 500\n",
      "2520 =======Fadein==== [-28.54296875, 39412.10546875, -39447.734375, 7.085182189941406] [67086.109375, 67071.8125, 14.29580307006836] 500 500\n",
      "2530 =======Fadein==== [4.4437255859375, 1500.1998291015625, -1495.820556640625, 0.06439314782619476] [3215.555908203125, 3187.092529296875, 28.463275909423828] 500 500\n",
      "2540 =======Fadein==== [-3.166015625, 1261.041259765625, -1264.4625244140625, 0.2552102506160736] [5911.728515625, 5895.21875, 16.50985336303711] 500 500\n",
      "2550 =======Fadein==== [-10.0390625, 19069.478515625, -19079.58203125, 0.06408679485321045] [40132.80078125, 40112.59375, 20.207178115844727] 500 500\n",
      "2560 =======Fadein==== [8.08349609375, 2817.21240234375, -2809.47314453125, 0.3441196084022522] [65.63552856445312, 40.283695220947266, 25.351835250854492] 500 500\n",
      "2570 =======Fadein==== [-21.27490234375, 4276.7109375, -4298.330078125, 0.3441762924194336] [11063.45703125, 11035.1533203125, 28.304149627685547] 500 500\n",
      "2580 =======Fadein==== [-3.587890625, 18601.466796875, -18605.12109375, 0.06674731522798538] [22338.326171875, 22317.6640625, 20.661468505859375] 500 500\n",
      "2590 =======Fadein==== [-65.8203125, 68579.421875, -68652.7109375, 7.465063095092773] [106212.28125, 106183.4375, 28.847414016723633] 500 500\n"
     ]
    }
   ],
   "source": [
    "GAN.train(\n",
    "    train_finger  \n",
    "    , train_face\n",
    "    , val_finger\n",
    "    , val_face\n",
    "    , batch_size = 4\n",
    "    , epochs = 10000\n",
    "    , run_folder = './model_save'\n",
    "    , print_every_n_batches = 10\n",
    "    , n_critic = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
