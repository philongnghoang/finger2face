{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop,SGD\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "class Cut_VGG19:\n",
    "    \"\"\"\n",
    "    Class object that fetches keras' VGG19 model trained on the imagenet dataset\n",
    "    and declares <layers_to_extract> as output layers. Used as feature extractor\n",
    "    for the perceptual loss function.\n",
    "    Args:\n",
    "        layers_to_extract: list of layers to be declared as output layers.\n",
    "        patch_size: integer, defines the size of the input (patch_size x patch_size).\n",
    "    Attributes:\n",
    "        loss_model: multi-output vgg architecture with <layers_to_extract> as output layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patch_size, layers_to_extract):\n",
    "        self.patch_size = patch_size\n",
    "        self.input_shape = (patch_size,) * 2 + (3,)\n",
    "        self.layers_to_extract = layers_to_extract\n",
    "        \n",
    "        if len(self.layers_to_extract) > 0:\n",
    "            self._cut_vgg()\n",
    "    \n",
    "    def _cut_vgg(self):\n",
    "        \"\"\"\n",
    "        Loads pre-trained VGG, declares as output the intermediate\n",
    "        layers selected by self.layers_to_extract.\n",
    "        \"\"\"\n",
    "        \n",
    "        vgg = VGG19(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "        vgg.trainable = False\n",
    "        outputs = [vgg.layers[i].output for i in self.layers_to_extract]\n",
    "        self.model = Model([vgg.input], outputs)\n",
    "\n",
    "        \n",
    "feature_extraction = Cut_VGG19(128,[5,9])   \n",
    "feature_extraction.model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        self.input_dim = (128,128,3)\n",
    "        self.optimiser = 'rmsprop'\n",
    "        self.z_dim = 128\n",
    "        ########################################\n",
    "        self.conditional_input = (128,128,1)\n",
    "        self.conditional_conv_filters = [2,4,16,32,64]\n",
    "        self.conditional_conv_kernel_size = [3,3,3,3,3]\n",
    "        self.conditional_conv_strides = [2,2,2,2,2]\n",
    "        #########################################\n",
    "        self.generator_initial_dense_layer_size = (4,4, 128)\n",
    "        self.generator_upsample = [1,1,1,1,1]\n",
    "        self.generator_conv_filters = [128,64,32,16,3]\n",
    "        self.generator_conv_kernel_size = [3,3,3,3,3]\n",
    "        self.generator_conv_strides = [2,2,2,2,2]\n",
    "        self.generator_batch_norm_momentum =  0.8\n",
    "        self.generator_activation = 'leaky_relu'\n",
    "        self.generator_dropout_rate = None\n",
    "        self.generator_learning_rate = 1e-3\n",
    "        ###########################################\n",
    "        self.discriminator_conv_filters = [16,32,64,128,256]\n",
    "        self.discriminator_conv_kernel_size = [3,3,3,3,3]\n",
    "        self.discriminator_conv_strides = [2,2,2,2,2]\n",
    "        self.discriminator_batch_norm_momentum = None\n",
    "        self.discriminator_activation = 'leaky_relu'\n",
    "        self.discriminator_dropout_rate = None\n",
    "        self.discriminator_learning_rate = 1e-3\n",
    "        ###########################################\n",
    "        self.weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "        self.grad_weight = 10\n",
    "        self.batch_size = 128\n",
    "        \n",
    "        self.n_layers_discriminator = len(self.discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(self.generator_conv_filters)\n",
    "        self.n_layers_conditional = len(self.conditional_conv_filters)\n",
    "        ###############################################                               \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        self.epoch = 0\n",
    "        self._build_generator()\n",
    "        self.generator.summary()\n",
    "        self._build_discriminator()\n",
    "        self.discriminator.summary()\n",
    "        print(\"#############################################\")\n",
    "        self._build_adversarial()\n",
    "        self.model.summary()\n",
    "    ####################### Loss ###########################\n",
    "    def wasserstein(self, y_true, y_pred):\n",
    "        return -K.mean(y_true * y_pred)\n",
    "    def get_perceptual_loss(self,y_true,y_pred):\n",
    "        content_feature = feature_extraction.model(y_true)\n",
    "        new_feature = feature_extraction.model(y_pred)\n",
    "        perceptual_loss = 0\n",
    "        weight = tf.constant([1/16,1/8], dtype = tf.float32)\n",
    "        for i in range(len(new_feature)):\n",
    "            perceptual_loss += weight[i]*K.mean(K.square(new_feature[i] - content_feature[i]))\n",
    "        l2_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true,y_pred))\n",
    "        total_loss = perceptual_loss + 150*l2_loss\n",
    "        return total_loss\n",
    "\n",
    "    \n",
    "    ################# Activation layer #####################                                                                \n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu':\n",
    "            layer = LeakyReLU(alpha = 0.2)\n",
    "        else:\n",
    "            layer = Activation(activation)\n",
    "        return layer\n",
    "    \n",
    "    ####################################################################\n",
    "    #################### Build Discriminator Model #####################\n",
    "    ####################################################################\n",
    "    def _build_discriminator(self):\n",
    "        discriminator_input = Input(shape=self.input_dim, name='discriminator_input')\n",
    "        target = Input(shape= self.conditional_input, name='target_image')\n",
    "        x = Concatenate()([discriminator_input,target])\n",
    "        #x = critic_input\n",
    "\n",
    "        for i in range(self.n_layers_discriminator):\n",
    "            x = Conv2D(\n",
    "                filters = self.discriminator_conv_filters[i]\n",
    "                , kernel_size = self.discriminator_conv_kernel_size[i]\n",
    "                , strides = self.discriminator_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'discriminator_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "\n",
    "            if self.discriminator_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n",
    "            x = self.get_activation(self.discriminator_activation)(x)\n",
    "            if self.discriminator_dropout_rate:\n",
    "                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        discriminator_output = Dense(1, activation=None\n",
    "        , kernel_initializer = self.weight_init\n",
    "        )(x)\n",
    "\n",
    "        self.discriminator = Model([discriminator_input, target], discriminator_output)\n",
    "        \n",
    "    ####################################################################\n",
    "    #################### Build Generator Model #########################\n",
    "    ####################################################################\n",
    "    \n",
    "    def _build_generator(self):\n",
    "        ############  generator ###############\n",
    "        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n",
    "        x = generator_input\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        if self.generator_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)       \n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "        ########### Conditional #############\n",
    "        conditional_input = Input(shape=self.conditional_input, name='conditional_input')\n",
    "        x_label = conditional_input\n",
    "        for i in range(self.n_layers_conditional):\n",
    "            x_label = Conv2D(\n",
    "                filters = self.conditional_conv_filters[i]\n",
    "                , kernel_size = self.conditional_conv_filters[i]\n",
    "                , strides = self.conditional_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'conditional_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x_label)\n",
    "            x_label = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x_label)\n",
    "            x_label = self.get_activation(self.generator_activation)(x_label)\n",
    "        ########### Concatenate ############\n",
    "        x = Concatenate()([x, x_label])  \n",
    "        \n",
    "        if self.generator_dropout_rate:\n",
    "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "\n",
    "        for i in range(self.n_layers_generator):\n",
    "\n",
    "            if self.generator_upsample[i] == 2:\n",
    "                x = UpSampling2D()(x)\n",
    "                x = Conv2D(\n",
    "                filters = self.generator_conv_filters[i]\n",
    "                , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'generator_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "            else:\n",
    "\n",
    "                x = Conv2DTranspose(\n",
    "                    filters = self.generator_conv_filters[i]\n",
    "                    , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                    , padding = 'same'\n",
    "                    , strides = self.generator_conv_strides[i]\n",
    "                    , name = 'generator_conv_' + str(i)\n",
    "                    , kernel_initializer = self.weight_init\n",
    "                    )(x)\n",
    "\n",
    "            if i < self.n_layers_generator - 1:\n",
    "\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "                x = self.get_activation(self.generator_activation)(x)\n",
    "                \n",
    "            else:\n",
    "                x = Activation('tanh')(x)\n",
    "\n",
    "        generator_output = x\n",
    "        self.generator = Model([generator_input,conditional_input], generator_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "\n",
    "\n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val\n",
    "        for l in m.layers:\n",
    "            l.trainable = val\n",
    "\n",
    "    def _build_adversarial(self):\n",
    "                \n",
    "        self.discriminator.compile(\n",
    "            optimizer=self.get_opti(self.discriminator_learning_rate) \n",
    "            , loss = self.wasserstein\n",
    "        )\n",
    "        \n",
    "        ### COMPILE THE FULL GAN\n",
    "\n",
    "        self.set_trainable(self.discriminator, False)\n",
    "        \n",
    "        gen_noise_input, gen_label_input = self.generator.input\n",
    "        gen_fake_image_output = self.generator.output\n",
    "\n",
    "        disc_output = self.discriminator([gen_fake_image_output, gen_label_input])\n",
    "        \n",
    "        self.model = Model([gen_noise_input, gen_label_input], [disc_output,gen_fake_image_output])\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer = self.get_opti(self.generator_learning_rate)\n",
    "            , loss=[self.wasserstein,self.get_perceptual_loss]\n",
    "            )\n",
    "        \n",
    "        \n",
    "#         self.generator.compile(\n",
    "#             optimizer=self.get_opti(self.generator_learning_rate)\n",
    "#             , loss=get_perceptual_loss\n",
    "#             )\n",
    "\n",
    "        self.set_trainable(self.discriminator, True)\n",
    "        \n",
    "    def train_discriminator(self, x_train,Y_train,label_train, batch_size, using_generator):\n",
    "\n",
    "        valid = np.ones((batch_size,1))\n",
    "        fake = -np.ones((batch_size,1))\n",
    "\n",
    "        if using_generator:\n",
    "            true_imgs = next(Y_train)\n",
    "            if true_imgs.shape[0] != batch_size:\n",
    "                true_imgs = next(Y_train)\n",
    "        else:\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size)\n",
    "            true_imgs = Y_train[idx]\n",
    "        \n",
    "        \n",
    "        #noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        noise = next(x_train)\n",
    "        label = next(label_train)\n",
    "        \n",
    "        gen_imgs = self.generator.predict([noise,label])\n",
    "        \n",
    "        #g_perceptual = self.generator.train_on_batch([noise,label],true_imgs)\n",
    "        \n",
    "        d_loss_real =   self.discriminator.train_on_batch([true_imgs,label], valid)\n",
    "        d_loss_fake =   self.discriminator.train_on_batch([gen_imgs,label], fake)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "        for l in self.discriminator.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "        for l in self.discriminator.layers:\n",
    "        \n",
    "            weights = l.get_weights()\n",
    "            if 'batch_normalization' in l.get_config()['name']:\n",
    "                pass\n",
    "                # weights = [np.clip(w, -0.01, 0.01) for w in weights[:2]] + weights[2:]\n",
    "            else:\n",
    "                weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            \n",
    "            l.set_weights(weights)\n",
    "\n",
    "        return [d_loss, d_loss_real, d_loss_fake]\n",
    "\n",
    "    def train_generator(self,x_train,Y_train,label_train, batch_size):\n",
    "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "        noise = next(x_train)\n",
    "        label = next(label_train)\n",
    "        true_images = next(Y_train)\n",
    "        #noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return self.model.train_on_batch([noise,label], [valid,true_images])\n",
    "\n",
    "    def load_random_batch(self,X_train,Y_train,batch_size):\n",
    "        num_image = X_train.shape[0]\n",
    "        random_samples_indices = np.random.choice(num_image, batch_size,replace=False)\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in random_samples_indices:\n",
    "            X.append(X_train[i])\n",
    "            Y.append(Y_train[i])\n",
    "        X = iter(np.asarray(X))\n",
    "        Y = iter(np.asarray(Y))\n",
    "        return X,Y\n",
    "    \n",
    "    def shuffle_data_batch(self,array_X,array_Y,array_label,batch_size):\n",
    "        indices = np.arange(array_X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        array_X = array_X[indices]\n",
    "        array_Y = array_Y[indices]\n",
    "        array_label = array_label[indices]\n",
    "        def split_into_chunks(l, n):\n",
    "            for i in range(0, l.shape[0], n):\n",
    "                yield l[i:i + n]  \n",
    "        array_X = split_into_chunks(array_X,batch_size)\n",
    "        array_Y = split_into_chunks(array_Y,batch_size)\n",
    "        array_label = split_into_chunks(array_label,batch_size)\n",
    "        return array_X,array_Y,array_label\n",
    "\n",
    "    def train(self, x_train,Y_train,x_val,Y_val,label_train,label_val, batch_size, epochs, run_folder, print_every_n_batches = 10, n_critic = 5,using_generator = False):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for epoch in range(self.epoch, self.epoch + epochs):\n",
    "            x_train_gan,Y_train_gan,label_train_gan = self.shuffle_data_batch(x_train,Y_train,label_train,batch_size)\n",
    "            for _ in range(n_critic):\n",
    "                d_loss = self.train_discriminator(x_train_gan,Y_train_gan,label_train_gan, batch_size, using_generator)\n",
    "\n",
    "            g_loss = self.train_generator(x_train_gan,Y_train_gan,label_train_gan, batch_size)\n",
    "               \n",
    "            # Plot the progress\n",
    "            \n",
    "            self.d_losses.append(d_loss)\n",
    "            self.g_losses.append(g_loss)\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % print_every_n_batches == 0:\n",
    "                print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)]  [W loss: %.3f, G loss: %.3f] \" % (epoch, d_loss[0], d_loss[1], d_loss[2], g_loss[0], g_loss[1]))\n",
    "                self.sample_images(x_val,Y_val,label_val,run_folder)\n",
    "                #self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n",
    "                #self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "                #self.save_model(run_folder)\n",
    "            \n",
    "            self.epoch+=1\n",
    "\n",
    "\n",
    "\n",
    "    def sample_images(self,x_val,Y_val,label_val, run_folder):\n",
    "        # Test\n",
    "        r, c = 4, 4\n",
    "\n",
    "        latent_code = x_val[:100,:]\n",
    "        y_true = Y_val[:100,:,:,:]\n",
    "        label_true = label_val[:100,:,:,:]\n",
    "        gen_imgs = self.generator.predict([latent_code,label_true])\n",
    "        # Perceptual loss\n",
    "        #perceptloss = get_perceptual_loss(y_true,gen_imgs)\n",
    "\n",
    "        indx = np.random.choice(y_true.shape[0], int(0.5*c*r) ,replace=False)\n",
    "\n",
    "        face_real = 0.5*(y_true[indx]+1)\n",
    "        face_real = face_real[:,:,:,[2,1,0]]\n",
    "\n",
    "        gen_imgs = 0.5 * (gen_imgs[indx] + 1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "        gen_imgs = gen_imgs[:,:,:,[2,1,0]]\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "        #fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss))\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(int(0.5*c)):\n",
    "                axs[i,2*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "                axs[i,2*j].axis('off')\n",
    "                axs[i,2*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "                axs[i,2*j+1].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(os.path.join(run_folder, \"images_latent/sample_%d.png\" % self.epoch))\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN = WGANGP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_train/data_split_v2/'\n",
    "with open(os.path.join(DATA_DIR,\"data_train_face_train_3channels.pkl\"), \"rb\") as input_file:\n",
    "    data_train_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"latentcode_finger_train_3channels.pkl\"), \"rb\") as input_file:\n",
    "    data_train_latentcode = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_train_finger_train_3channels.pkl\"), \"rb\") as input_file:\n",
    "    data_train_finger = pickle.load(input_file)\n",
    "\n",
    "    \n",
    "with open(os.path.join(DATA_DIR,\"data_train_face_val_3channels.pkl\"), \"rb\") as input_file:\n",
    "    data_val_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"latentcode_finger_val_3channels.pkl\"), \"rb\") as input_file:\n",
    "    data_val_latentcode = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_train_finger_val_3channels.pkl\"), \"rb\") as input_file:\n",
    "    data_val_finger = pickle.load(input_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "def resize_data(data):\n",
    "    images_arr = []\n",
    "    for img in data:\n",
    "        #print(img.shape)\n",
    "        img = array_to_img(img)\n",
    "        resized_img = img.resize(size=(128, 128))\n",
    "        images_arr.append(img_to_array(resized_img))\n",
    "    data = np.asarray(images_arr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_finger = resize_data(data_train_finger)\n",
    "data_val_finger = resize_data(data_val_finger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0 16.0 0.0\n",
      "255.0 148.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(data_train_finger),np.median(data_train_finger),np.min(data_train_finger))\n",
    "print(np.max(data_train_face),np.median(data_train_face),np.min(data_train_face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_finger = ((data_train_finger)/np.max(data_train_finger))\n",
    "data_val_finger = ((data_val_finger)/np.max(data_val_finger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_finger = np.where(data_train_finger > .5, 1.0, -1.0).astype('float32')\n",
    "data_val_finger = np.where(data_val_finger > .5, 1.0, -1.0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1621, 128, 128, 1), (150, 128, 128, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_finger.shape, data_val_finger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_face = ((data_train_face-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1621, 128, 128, 3), (1621, 128))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_face.shape,data_train_latentcode[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_face = ((data_val_face-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 128, 128, 3), (150, 128))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_face.shape,data_val_latentcode[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -1.0 -1.0\n",
      "1.0 0.16078432 -1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(data_train_finger),np.median(data_train_finger),np.min(data_train_finger))\n",
    "print(np.max(data_train_face),np.median(data_train_face),np.min(data_train_face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latentcode = data_train_latentcode[2]\n",
    "train_face = data_train_face\n",
    "valid_latentcode = data_val_latentcode[2]\n",
    "valid_face = data_val_face\n",
    "label_train_finger = data_train_finger\n",
    "label_val_finger = data_val_finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_latentcode,valid_latentcode,train_face,valid_face = train_test_split(data_train_latentcode[2],\n",
    "#                                                              data_train_face,\n",
    "#                                                              test_size=0.2,\n",
    "#                                                              random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1621, 128), (150, 128), (1621, 128, 128, 3), (150, 128, 128, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_latentcode.shape,valid_latentcode.shape,train_face.shape,valid_face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data_train_finger[10],cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(data_train_face[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GAN.train(  \n",
    "      train_latentcode\n",
    "    , train_face\n",
    "    , valid_latentcode\n",
    "    , valid_face\n",
    "    , label_train_finger\n",
    "    , label_val_finger\n",
    "    , batch_size = 128\n",
    "    , epochs = 10000\n",
    "    , run_folder = './model_save'\n",
    "    , print_every_n_batches = 50\n",
    "    , n_critic = 5\n",
    "    , using_generator = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN.discriminator.save('./model_save/model/discriminator_3channel_new.h5')\n",
    "GAN.generator.save('./model_save/model/generator_3channel_new.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = tf.keras.models.load_model('./model_save/model/generator_3channel_conditional.h5',custom_objects={'get_perceptual_loss': get_perceptual_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(validate=True):    # Test\n",
    "    r, c = 4, 4\n",
    "    if validate :\n",
    "        latent_code = valid_latentcode[:150,:]\n",
    "        y_true = valid_face[:150,:,:,:]\n",
    "        label_true = label_val_finger[:150,:,:,:]\n",
    "    else:\n",
    "        latent_code = train_latentcode[900:1000,:]\n",
    "        y_true = train_face[900:1000,:,:,:]\n",
    "        label_true = label_train_finger[900:1000,:,:,:]\n",
    "    \n",
    "    gen_imgs = GAN.generator.predict([latent_code,label_true])\n",
    "    # Perceptual loss\n",
    "    #perceptloss = get_perceptual_loss(y_true,gen_imgs)\n",
    "\n",
    "    indx = np.random.choice(y_true.shape[0], int(0.5*c*r) ,replace=False)\n",
    "\n",
    "    face_real = 0.5*(y_true[indx]+1)\n",
    "    face_real = face_real[:,:,:,[2,1,0]]\n",
    "\n",
    "    gen_imgs = 0.5 * (gen_imgs[indx] + 1)\n",
    "    gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "    gen_imgs = gen_imgs[:,:,:,[2,1,0]]\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "    #fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(int(0.5*c)):\n",
    "            axs[i,2*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "            axs[i,2*j].axis('off')\n",
    "            axs[i,2*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "            axs[i,2*j+1].axis('off')\n",
    "            cnt += 1\n",
    "\n",
    "    plt.show()\n",
    "    #fig.savefig(os.path.join('./model_save', \"images/face_gens.png\" ))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAN.sample_images(train_latentcode,train_face,\"./model_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_train/data_split/'\n",
    "with open(os.path.join(DATA_DIR,\"latentcode_val_augment.pkl\"), \"rb\") as input_file:\n",
    "    latentcode_augment = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"face_val_augment.pkl\"), \"rb\") as input_file:\n",
    "    face_augment = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"finger_val_augment.pkl\"), \"rb\") as input_file:\n",
    "    finger_augment = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 128), (300, 128, 128, 3), (300, 128, 128, 1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latentcode_augment[2].shape,face_augment.shape,finger_augment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "finger_augment = ((finger_augment)/np.max(finger_augment))\n",
    "finger_augment = np.where(finger_augment > .5, 1.0, -1.0).astype('float32')\n",
    "#finger_augment = ((finger_augment-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 8, 4\n",
    "\n",
    "latentcode_datatrain = train_latentcode[200:500,:]\n",
    "y_true_datatrain = train_face[200:500,:,:,:]\n",
    "label_true_datatrain = label_train_finger[200:500,:,:,:]\n",
    "print(latentcode_datatrain.shape)\n",
    "noise_gauss = tf.keras.backend.random_normal(shape=(300, 128))\n",
    "gen_imgs_augment = GAN.generator.predict([latentcode_augment[2]+noise_gauss*0.2,finger_augment])\n",
    "\n",
    "# Perceptual loss\n",
    "perceptloss_augment = get_perceptual_loss(y_true_datatrain,gen_imgs_augment)\n",
    "\n",
    "indx = np.random.choice(y_true_datatrain.shape[0], int(0.5*c*r) ,replace=False)\n",
    "\n",
    "face_real = 0.5*(y_true_datatrain[indx]+1)\n",
    "face_real = face_real[:,:,:,[2,1,0]]\n",
    "\n",
    "gen_imgs = 0.5 * (gen_imgs_augment[indx] + 1)\n",
    "gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "gen_imgs = gen_imgs[:,:,:,[2,1,0]]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15,30))\n",
    "#fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss_augment))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(int(0.25*c)):\n",
    "        axs[i,4*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "        axs[i,4*j].axis('off')\n",
    "        axs[i,4*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "        axs[i,4*j+1].axis('off')\n",
    "        axs[i,4*j+2].imshow(np.squeeze(finger_augment[indx][cnt, :,:,:]),cmap='gray')\n",
    "        axs[i,4*j+2].axis('off')\n",
    "        axs[i,4*j+3].imshow(np.squeeze(label_true_datatrain[indx][cnt, :,:,:]),cmap='gray')\n",
    "        axs[i,4*j+3].axis('off')\n",
    "    cnt += 1\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(os.path.join('./model_save', \"images/face_gens_1.png\" ))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
