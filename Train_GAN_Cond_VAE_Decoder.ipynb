{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop,SGD\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "#from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "# tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "class Cut_VGG19:\n",
    "    \"\"\"\n",
    "    Class object that fetches keras' VGG19 model trained on the imagenet dataset\n",
    "    and declares <layers_to_extract> as output layers. Used as feature extractor\n",
    "    for the perceptual loss function.\n",
    "    Args:\n",
    "        layers_to_extract: list of layers to be declared as output layers.\n",
    "        patch_size: integer, defines the size of the input (patch_size x patch_size).\n",
    "    Attributes:\n",
    "        loss_model: multi-output vgg architecture with <layers_to_extract> as output layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patch_size, layers_to_extract):\n",
    "        self.patch_size = patch_size\n",
    "        self.input_shape = (patch_size,) * 2 + (3,)\n",
    "        self.layers_to_extract = layers_to_extract\n",
    "        \n",
    "        if len(self.layers_to_extract) > 0:\n",
    "            self._cut_vgg()\n",
    "    \n",
    "    def _cut_vgg(self):\n",
    "        \"\"\"\n",
    "        Loads pre-trained VGG, declares as output the intermediate\n",
    "        layers selected by self.layers_to_extract.\n",
    "        \"\"\"\n",
    "        \n",
    "        vgg = VGG19(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "        vgg.trainable = False\n",
    "        outputs = [vgg.layers[i].output for i in self.layers_to_extract]\n",
    "        self.model = Model([vgg.input], outputs)\n",
    "\n",
    "        \n",
    "feature_extraction = Cut_VGG19(256,[5,9])   \n",
    "feature_extraction.model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        self.input_dim = (256,256,3)\n",
    "        self.optimiser = 'rmsprop'\n",
    "        self.z_dim = 256\n",
    "        ################ Encoder Model ########################\n",
    "        self.encoder_input = (256,256,1)\n",
    "        self.encoder_conv_filters = [64,128,256,512]\n",
    "        self.encoder_conv_kernel_size = [3,3,3,3]\n",
    "        self.encoder_conv_strides = [2,2,2,3]\n",
    "        self.encoder_batch_norm_momentum =  None\n",
    "        self.encoder_activation = 'relu'\n",
    "        self.encoder_dropout_rate = None\n",
    "        self.encoder_learning_rate = 1e-5\n",
    "        \n",
    "        ################ Decoder Model ########################\n",
    "        self.decoder_initial_dense_layer_size = (16,16,512)\n",
    "        self.decoder_conv_filters = [256,128,64,1]\n",
    "        self.decoder_conv_kernel_size = [3,3,3,3]\n",
    "        self.decoder_conv_strides = [2,2,2,2]\n",
    "        self.decoder_batch_norm_momentum =  None\n",
    "        self.decoder_activation = 'relu'\n",
    "        self.decoder_dropout_rate = None\n",
    "        self.decoder_learning_rate = 1e-5\n",
    "\n",
    "        ################ Generator Model #########################\n",
    "        self.generator_initial_dense_layer_size = (4,4,256)\n",
    "        self.generator_upsample = [1,1,1,1,1,1]\n",
    "        self.generator_conv_filters = [128,64,32,16,8,3]\n",
    "        self.generator_conv_kernel_size = [3,3,3,3,3,3]\n",
    "        self.generator_conv_strides = [2,2,2,2,2,2]\n",
    "        self.generator_batch_norm_momentum =  0.8\n",
    "        self.generator_activation = 'leaky_relu'\n",
    "        self.generator_dropout_rate = None\n",
    "        self.generator_learning_rate = 1e-3\n",
    "        ################ Discriminator Model ###########################\n",
    "        self.discriminator_conv_filters = [8,16,32,64,128,256]\n",
    "        self.discriminator_conv_kernel_size = [3,3,3,3,3,3]\n",
    "        self.discriminator_conv_strides = [2,2,2,2,2,2]\n",
    "        self.discriminator_batch_norm_momentum = None\n",
    "        self.discriminator_activation = 'leaky_relu'\n",
    "        self.discriminator_dropout_rate = None\n",
    "        self.discriminator_learning_rate = 1e-3\n",
    "        ###########################################\n",
    "        self.weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "        self.grad_weight = 10\n",
    "        self.batch_size = 128\n",
    "        ############################################\n",
    "        self.n_layers_encoder = len(self.encoder_conv_filters)\n",
    "        self.n_layers_decoder = len(self.decoder_conv_filters)\n",
    "        self.n_layers_discriminator = len(self.discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(self.generator_conv_filters)\n",
    "        ###############################################                               \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        self.epoch = 0\n",
    "        ###############################################\n",
    "        self._build_encoder()\n",
    "        self.encoder_model.summary()\n",
    "        self._build_decoder()\n",
    "        self.decoder_model.summary()\n",
    "        ###############################################\n",
    "        self._build_generator()\n",
    "        self.generator.summary()\n",
    "        self._build_discriminator()\n",
    "        self.discriminator.summary()\n",
    "        ###############################################\n",
    "        self._build_adversarial()\n",
    "        self.gan_model.summary()\n",
    "        self.vae_model.summary()\n",
    "        self.model.summary()\n",
    "        LOG_DIR = \"./logs/vae_gan.log\"\n",
    "        logging.basicConfig(filename=LOG_DIR,  \n",
    "                    level=logging.DEBUG,\n",
    "                    format=\"[%(asctime)s] [%(name)s] [%(message)s]\",\n",
    "                    filemode=\"w\")\n",
    "    ####################### Loss ###########################\n",
    "    def wasserstein(self, y_true, y_pred):\n",
    "        alpha = 100\n",
    "        return -K.mean(y_true * y_pred)*alpha\n",
    "    \n",
    "    def get_perceptual_loss(self,y_true,y_pred):\n",
    "        content_feature = feature_extraction.model(y_true)\n",
    "        new_feature = feature_extraction.model(y_pred)\n",
    "        perceptual_loss = 0\n",
    "        weight = tf.constant([1/16,1/8], dtype = tf.float32)\n",
    "        for i in range(len(new_feature)):\n",
    "            perceptual_loss += weight[i]*K.mean(K.square(new_feature[i] - content_feature[i]))\n",
    "        l2_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true,y_pred))\n",
    "        total_loss = perceptual_loss + 100*l2_loss\n",
    "        return total_loss\n",
    "\n",
    "    def log_normal_pdf(self,sample, mean, logvar, raxis=1):\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        return tf.reduce_sum(\n",
    "          -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "          axis=raxis)\n",
    "    \n",
    "    def VAE_reconstruct_loss(self, y_true, y_pred):\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y_true)\n",
    "        logpx_z = -tf.reduce_mean(cross_ent, axis=[1, 2, 3])\n",
    "        return logpx_z\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def VAE_MC_loss(self,y_true, y_pred):\n",
    "        mean_true = y_true[:,:256]\n",
    "        log_var_true = y_true[:,256:]\n",
    "        z_mean = y_pred[:,:256]\n",
    "        z_log_var = y_pred[:,256:]\n",
    "        z = self.reparameterize(z_mean,z_log_var)\n",
    "        logpz = self.log_normal_pdf(z, mean_true, log_var_true)\n",
    "        logqz_x = self.log_normal_pdf(z, z_mean, z_log_var)\n",
    "        return -tf.reduce_mean(logpz - logqz_x) \n",
    "    \n",
    "        \n",
    "    ################# Activation layer #####################                                                                \n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu':\n",
    "            layer = LeakyReLU(alpha = 0.2)\n",
    "        else:\n",
    "            layer = Activation(activation)\n",
    "        return layer\n",
    "    ####################################################################\n",
    "    #################### Build Encoder Model ###########################\n",
    "    ####################################################################\n",
    "    def _build_encoder(self):\n",
    "        encoder_input_layer = Input(shape=self.encoder_input, name='encoder_input')\n",
    "        x = encoder_input_layer\n",
    "        for i in range(self.n_layers_encoder):\n",
    "            x = Conv2D(\n",
    "                filters = self.encoder_conv_filters[i]\n",
    "                , kernel_size = self.encoder_conv_kernel_size[i]\n",
    "                , strides = self.encoder_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'encoder_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                ,activation='relu'\n",
    "                )(x)\n",
    "\n",
    "            if self.encoder_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum = self.encoder_batch_norm_momentum)(x)\n",
    "            #x = self.get_activation(self.encoder_activation)(x)\n",
    "            if self.encoder_dropout_rate:\n",
    "                x = Dropout(rate = self.encoder_dropout_rate)(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "#         encoder_output = Dense(1, activation=None\n",
    "#         , kernel_initializer = self.weight_init\n",
    "#         )(x)\n",
    "        z_mean = Dense(self.z_dim, name=\"z_mean\",kernel_initializer = self.weight_init)(x)\n",
    "        z_log_var = Dense(self.z_dim, name=\"z_log_var\", kernel_initializer = self.weight_init)(x)\n",
    "    \n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        encoder_output = [z,z_mean,z_log_var]\n",
    "        self.encoder_model = Model(encoder_input_layer,encoder_output ,name=\"Encoder\")\n",
    "    ####################################################################\n",
    "    #################### Build Decoder Model ###########################\n",
    "    ####################################################################\n",
    "    def _build_decoder(self):\n",
    "        decoder_input_layer = Input(shape=(self.z_dim,), name='decoder_input')\n",
    "        x = decoder_input_layer\n",
    "        x = Dense(np.prod(self.decoder_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "        if self.decoder_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum = self.decoder_batch_norm_momentum)(x)       \n",
    "        x = self.get_activation(self.decoder_activation)(x)\n",
    "        x = Reshape(self.decoder_initial_dense_layer_size)(x)\n",
    "        for i in range(self.n_layers_decoder):\n",
    "            if i == self.n_layers_decoder-1:\n",
    "                x = Conv2DTranspose(\n",
    "                filters = self.decoder_conv_filters[i]\n",
    "                , kernel_size = self.decoder_conv_kernel_size[i]\n",
    "                , strides = self.decoder_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'decoder_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                ,activation='tanh'\n",
    "                )(x)\n",
    "            else:\n",
    "                x = Conv2DTranspose(\n",
    "                    filters = self.decoder_conv_filters[i]\n",
    "                    , kernel_size = self.decoder_conv_kernel_size[i]\n",
    "                    , strides = self.decoder_conv_strides[i]\n",
    "                    , padding = 'same'\n",
    "                    , name = 'decoder_conv_' + str(i)\n",
    "                    , kernel_initializer = self.weight_init\n",
    "                    ,activation='relu'\n",
    "                    )(x)\n",
    "                #x = self.get_activation(self.decoder_activation)(x)\n",
    "        \n",
    "        decoder_output = x\n",
    "        self.decoder_model = Model(decoder_input_layer,decoder_output)\n",
    "    ####################################################################\n",
    "    #################### Build Discriminator Model #####################\n",
    "    ####################################################################\n",
    "    def _build_discriminator(self):\n",
    "        \n",
    "        discriminator_input = Input(shape=self.input_dim, name='discriminator_input')\n",
    "        x = discriminator_input\n",
    "        \n",
    "        for i in range(self.n_layers_discriminator):\n",
    "            x = Conv2D(\n",
    "                filters = self.discriminator_conv_filters[i]\n",
    "                , kernel_size = self.discriminator_conv_kernel_size[i]\n",
    "                , strides = self.discriminator_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'discriminator_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "\n",
    "            if self.discriminator_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n",
    "            x = self.get_activation(self.discriminator_activation)(x)\n",
    "            if self.discriminator_dropout_rate:\n",
    "                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n",
    "\n",
    "                \n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        discriminator_output = Dense(1, activation=None\n",
    "        , kernel_initializer = self.weight_init\n",
    "        )(x)\n",
    "        x = self.get_activation('sigmoid')(x)\n",
    "        self.discriminator = Model(discriminator_input, discriminator_output,name=\"Discriminator\")\n",
    "        \n",
    "    ####################################################################\n",
    "    #################### Build Generator Model #########################\n",
    "    ####################################################################\n",
    "    \n",
    "    def _build_generator(self):\n",
    "        ############  generator ###############\n",
    "        generator_input_layer = Input(shape=(self.z_dim,), name='generator_input')\n",
    "        x = generator_input_layer\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        if self.generator_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)       \n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "        \n",
    "        if self.generator_dropout_rate:\n",
    "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "\n",
    "        for i in range(self.n_layers_generator):\n",
    "\n",
    "            if self.generator_upsample[i] == 2:\n",
    "                x = UpSampling2D()(x)\n",
    "                x = Conv2D(\n",
    "                filters = self.generator_conv_filters[i]\n",
    "                , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'generator_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "            else:\n",
    "\n",
    "                x = Conv2DTranspose(\n",
    "                    filters = self.generator_conv_filters[i]\n",
    "                    , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                    , padding = 'same'\n",
    "                    , strides = self.generator_conv_strides[i]\n",
    "                    , name = 'generator_conv_' + str(i)\n",
    "                    , kernel_initializer = self.weight_init\n",
    "                    )(x)\n",
    "\n",
    "            if i < self.n_layers_generator - 1:\n",
    "\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "                x = self.get_activation(self.generator_activation)(x)\n",
    "                \n",
    "            else:\n",
    "                x = Activation('tanh')(x)\n",
    "\n",
    "        generator_output = x\n",
    "        self.generator = Model(generator_input_layer, generator_output,name=\"Generator\")\n",
    "\n",
    "\n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "\n",
    "\n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val\n",
    "        for l in m.layers:\n",
    "            l.trainable = val\n",
    "\n",
    "    def _build_adversarial(self):\n",
    "                \n",
    "        self.discriminator.compile(\n",
    "            optimizer=self.get_opti(self.discriminator_learning_rate) \n",
    "            , loss = self.wasserstein\n",
    "        )\n",
    "        #######################################\n",
    "        self.set_trainable(self.discriminator, False)\n",
    "        input_gen = self.generator.input\n",
    "        gen_fake_image_output = self.generator(input_gen)\n",
    "        disc_output = self.discriminator(gen_fake_image_output)\n",
    "        self.gan_model = Model(input_gen, [disc_output, gen_fake_image_output])\n",
    "        #================ Model =========================\n",
    "        self.gan_model.compile(\n",
    "            optimizer = self.get_opti(self.encoder_learning_rate)\n",
    "            , loss=[self.wasserstein,self.get_perceptual_loss]\n",
    "            )\n",
    "\n",
    "        self.set_trainable(self.discriminator, True)\n",
    "        \n",
    "        ############ BUILD VAE ################\n",
    "        self.set_trainable(self.discriminator, False)\n",
    "        self.set_trainable(self.generator, False)\n",
    "        \n",
    "        input_encoder = self.encoder_model.input\n",
    "        encoder_output = self.encoder_model(input_encoder)\n",
    "        z,z_mean,z_logvar = encoder_output[0],encoder_output[1],encoder_output[2]\n",
    "        \n",
    "        output_decoder = self.decoder_model(z)\n",
    "        self.vae_model = Model(input_encoder,[output_decoder,tf.concat([z_mean,z_logvar],1)], name = \"vae_model\")\n",
    "        self.vae_model.compile(\n",
    "            optimizer=self.get_opti(self.encoder_learning_rate)\n",
    "            , loss=[self.VAE_reconstruct_loss,self.VAE_MC_loss]\n",
    "            )\n",
    "        self.set_trainable(self.discriminator, True)\n",
    "        self.set_trainable(self.generator, True)\n",
    "\n",
    "        ############# BUILD FULL MULTITASK MODEL\n",
    "        input_encoder = self.encoder_model.input\n",
    "        encoder_output = self.encoder_model(input_encoder)\n",
    "        \n",
    "        z,z_mean,z_logvar = encoder_output[0],encoder_output[1],encoder_output[2]\n",
    "        \n",
    "        output_decoder = self.decoder_model(z)\n",
    "        \n",
    "        gen_fake_image_output = self.generator(z)\n",
    "        \n",
    "        disc_output = self.discriminator(gen_fake_image_output)\n",
    "        \n",
    "        self.model = Model(input_encoder, [gen_fake_image_output,output_decoder,tf.concat([z_mean,z_logvar],1)])\n",
    "        #================ Model =========================\n",
    "        self.model.compile(\n",
    "            optimizer = self.get_opti(self.encoder_learning_rate)\n",
    "            , loss=[self.get_perceptual_loss,self.VAE_reconstruct_loss,self.VAE_MC_loss]\n",
    "            )\n",
    "\n",
    "        \n",
    "    def train_discriminator(self, x_train,Y_train, batch_size):\n",
    "\n",
    "        valid = np.ones((batch_size,1))\n",
    "        fake = -np.ones((batch_size,1))\n",
    "\n",
    "        input_imgs = x_train\n",
    "        true_imgs  = Y_train\n",
    "        \n",
    "        latent_code = self.encoder_model.predict(input_imgs)[0]\n",
    "        gen_imgs = self.generator.predict(latent_code)\n",
    "                \n",
    "        d_loss_real =   self.discriminator.train_on_batch(true_imgs, valid)\n",
    "        d_loss_fake =   self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "        for l in self.discriminator.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "        for l in self.discriminator.layers:\n",
    "        \n",
    "            weights = l.get_weights()\n",
    "            if 'batch_normalization' in l.get_config()['name']:\n",
    "                pass\n",
    "                # weights = [np.clip(w, -0.01, 0.01) for w in weights[:2]] + weights[2:]\n",
    "            else:\n",
    "                weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            \n",
    "            l.set_weights(weights)\n",
    "\n",
    "        return [d_loss, d_loss_real, d_loss_fake]\n",
    "\n",
    "    def train_generator(self,x_train,Y_train, batch_size):\n",
    "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "        true_images = Y_train\n",
    "        input_images = x_train\n",
    "        \n",
    "        latencode = self.encoder_model.predict(input_images)\n",
    "        gans_loss = self.gan_model.train_on_batch(latencode[0], [valid,true_images])\n",
    "        return gans_loss\n",
    "    \n",
    "    def train_vae(self,x_train,Y_train,batch_size):\n",
    "        \n",
    "        valid_z = np.zeros((batch_size,512),dtype=np.float32)\n",
    "        \n",
    "        return self.vae_model.train_on_batch(x_train,[x_train,valid_z])\n",
    "    \n",
    "    def train_full_model(self,x_train,Y_train,batch_size):\n",
    "        valid_z = np.zeros((batch_size,512),dtype=np.float32)\n",
    "        \n",
    "        return self.model.train_on_batch(x_train,[Y_train,x_train,valid_z])\n",
    "\n",
    "    \n",
    "    def shuffle_data_batch(self,array_X,array_Y,batch_size):\n",
    "        indices = np.arange(array_X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        array_X = array_X[indices]\n",
    "        array_Y = array_Y[indices]\n",
    "        \n",
    "        def split_into_chunks(l, n):\n",
    "            for i in range(0, l.shape[0], n):\n",
    "                yield l[i:i + n]  \n",
    "        array_X = split_into_chunks(array_X,batch_size)\n",
    "        array_Y = split_into_chunks(array_Y,batch_size)\n",
    "        \n",
    "        return array_X,array_Y\n",
    "\n",
    "        \n",
    "    def train(self,x_train,Y_train,x_val,Y_val, batch_size, epochs, run_folder, print_every_n_batches = 10, n_critic = 5,using_generator = False):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = int(x_train.shape[0]/self.batch_size)-1\n",
    "        for epoch in range(self.epoch, self.epoch + epochs):\n",
    "            x_train_gan,Y_train_gan = self.shuffle_data_batch(x_train,Y_train,batch_size)\n",
    "            \n",
    "            for step in range(self.n_steps):\n",
    "                x, Y = next(x_train_gan),next(Y_train_gan)\n",
    "                for _ in range(n_critic):\n",
    "                    d_loss = self.train_discriminator(x,Y, batch_size)\n",
    "                    indices = np.random.shuffle(np.arange(self.batch_size))\n",
    "                    x = np.squeeze(x[indices],axis=0)\n",
    "                    Y = np.squeeze(Y[indices],axis=0)\n",
    "                for _ in range(5):\n",
    "                    vae_loss = self.train_vae(x,Y,batch_size)\n",
    "                    indices = np.random.shuffle(np.arange(self.batch_size))\n",
    "                    x = np.squeeze(x[indices],axis=0)\n",
    "                    Y = np.squeeze(Y[indices],axis=0)\n",
    "                g_loss = self.train_generator(x,Y, batch_size)\n",
    "                model_loss = self.train_full_model(x,Y,batch_size)\n",
    "                # Plot the progress\n",
    "\n",
    "                self.d_losses.append(d_loss)\n",
    "                self.g_losses.append(g_loss)\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % print_every_n_batches == 0:\n",
    "                logging.info(json.dumps({'epoch':epoch,'d_loss':d_loss,'g_loss':g_loss,'vae_loss':vae_loss,'full_model_loss':model_loss})) \n",
    "                print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)]  [Gan loss: %.3f, Per loss: %.3f,W: %.3f ]\" % (epoch, d_loss[0], d_loss[1], d_loss[2], g_loss[0], g_loss[0], g_loss[1]))\n",
    "                print(\"%d [VAE loss: %.3f (Re %.3f Mc %.3f) Full %.3f Per %.3f Re %.3f Mc %.3f ]\"%(epoch,vae_loss[0],vae_loss[1],vae_loss[2],model_loss[0],model_loss[1],model_loss[2],model_loss[3]))\n",
    "                print(\"==========================================================================================\")\n",
    "                self.sample_images(x_val,Y_val,run_folder)\n",
    "\n",
    "            self.epoch+=1\n",
    "\n",
    "\n",
    "\n",
    "    def sample_images(self,x_val,Y_val, run_folder):\n",
    "        # Test\n",
    "        r, c = 8, 4\n",
    "\n",
    "        y_true = Y_val\n",
    "        input_model = x_val\n",
    "        \n",
    "        latent_code = self.encoder_model.predict(input_model)\n",
    "        gen_imgs = self.generator.predict(latent_code[0])\n",
    "        \n",
    "        gen_finger = self.decoder_model.predict(latent_code[0])\n",
    "        # Perceptual loss\n",
    "        #perceptloss = get_perceptual_loss(y_true,gen_imgs)\n",
    "\n",
    "        indx = np.random.choice(y_true.shape[0], int(0.25*c*r) ,replace=False)\n",
    "        finger_real = x_val[indx]\n",
    "        face_real = 0.5*(y_true[indx]+1)\n",
    "        \n",
    "        #face_real = face_real[:,:,:,[2,1,0]]\n",
    "\n",
    "        gen_imgs = 0.5 * (gen_imgs[indx] + 1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "        #gen_imgs = gen_imgs[:,:,:,[2,1,0]]\n",
    "        gen_finger = gen_finger[indx]\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,30))\n",
    "        #fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss))\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(int(0.25*c)):\n",
    "                axs[i,4*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "                axs[i,4*j].axis('off')\n",
    "                axs[i,4*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "                axs[i,4*j+1].axis('off')\n",
    "                axs[i,4*j+2].imshow(np.squeeze(gen_finger[cnt, :,:,:]),cmap =\"gray\")\n",
    "                axs[i,4*j+2].axis('off')\n",
    "                axs[i,4*j+3].imshow(np.squeeze(finger_real[cnt, :,:,:]),cmap =\"gray\")\n",
    "                axs[i,4*j+3].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)         (None, 128, 128, 64) 640         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 64, 64, 128)  73856       encoder_conv_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 32, 32, 256)  295168      encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 11, 11, 512)  1180160     encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 61952)        0           encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 256)          15859968    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 256)          15859968    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 256)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,269,760\n",
      "Trainable params: 33,269,760\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 131072)            33685504  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 32, 32, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 64, 64, 128)       295040    \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "decoder_conv_3 (Conv2DTransp (None, 256, 256, 1)       577       \n",
      "=================================================================\n",
      "Total params: 35,234,817\n",
      "Trainable params: 35,234,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2DTran (None, 8, 8, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2DTran (None, 16, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2DTran (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 64, 64, 16)        4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_4 (Conv2DTran (None, 128, 128, 8)       1160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 128, 8)       32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_5 (Conv2DTran (None, 256, 256, 3)       219       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 256, 3)       0         \n",
      "=================================================================\n",
      "Total params: 1,463,347\n",
      "Trainable params: 1,454,659\n",
      "Non-trainable params: 8,688\n",
      "_________________________________________________________________\n",
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 128, 128, 8)       224       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_4 (Conv2D (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_5 (Conv2D (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 397,649\n",
      "Trainable params: 397,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "Generator (Functional)       (None, 256, 256, 3)       1463347   \n",
      "_________________________________________________________________\n",
      "Discriminator (Functional)   (None, 1)                 397649    \n",
      "=================================================================\n",
      "Total params: 1,860,996\n",
      "Trainable params: 1,852,308\n",
      "Non-trainable params: 8,688\n",
      "_________________________________________________________________\n",
      "Model: \"vae_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder (Functional)            [(None, 256), (None, 33269760    encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "functional_3 (Functional)       (None, 256, 256, 1)  35234817    Encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 512)]        0           Encoder[0][1]                    \n",
      "                                                                 Encoder[0][2]                    \n",
      "==================================================================================================\n",
      "Total params: 68,504,577\n",
      "Trainable params: 68,504,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder (Functional)            [(None, 256), (None, 33269760    encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Generator (Functional)          (None, 256, 256, 3)  1463347     Encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "functional_3 (Functional)       (None, 256, 256, 1)  35234817    Encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 512)]        0           Encoder[1][1]                    \n",
      "                                                                 Encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 69,967,924\n",
      "Trainable params: 69,959,236\n",
      "Non-trainable params: 8,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GAN = WGANGP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../Finger_enhancement/data_cropped/data_train/'\n",
    "with open(os.path.join(DATA_DIR,\"data_face_3571_train.pkl\"), \"rb\") as input_file:\n",
    "    data_train_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_fingerprint_3571_train.pkl\"), \"rb\") as input_file:\n",
    "    data_train_finger = pickle.load(input_file)\n",
    "\n",
    "    \n",
    "with open(os.path.join(DATA_DIR,\"data_face_3571_val.pkl\"), \"rb\") as input_file:\n",
    "    data_val_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_fingerprint_3571_val.pkl\"), \"rb\") as input_file:\n",
    "    data_val_finger = pickle.load(input_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2856, 256, 256, 3),\n",
       " (2856, 256, 256, 1),\n",
       " (715, 256, 256, 3),\n",
       " (715, 256, 256, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_face.shape,data_train_finger.shape,data_val_face.shape,data_val_finger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0 0\n",
      "255 137.0 0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(data_train_finger),np.median(data_train_finger),np.min(data_train_finger))\n",
    "print(np.max(data_train_face),np.median(data_train_face),np.min(data_train_face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_face_fingerprint(data_face,data_finger, num_augment_percent=0.5):\n",
    "    num_data = data_face.shape[0]\n",
    "    num_data_augment = int(data_face.shape[0]*num_augment_percent)\n",
    "    index_data = np.random.choice(data_face.shape[0],num_data_augment,replace=False)\n",
    "    \n",
    "    data_aug_face = []\n",
    "    data_aug_finger = []\n",
    "    for i in index_data:\n",
    "        data_aug_face.append(data_face[i,:,:,:])\n",
    "        data_aug_finger.append(data_finger[i,:,:,:])\n",
    "    data_aug_face = np.asarray(data_aug_face)    \n",
    "    data_aug_finger = np.asarray(data_aug_finger) \n",
    "\n",
    "    # create image data augmentation generator\n",
    "    datagen_face = ImageDataGenerator(horizontal_flip=True)\n",
    "    datagen_face.fit(data_aug_face)\n",
    "    \n",
    "    datagen_finger = ImageDataGenerator(horizontal_flip=False,height_shift_range=0.1,width_shift_range=0.1,shear_range=0.5,rotation_range=20)\n",
    "    datagen_finger.fit(data_aug_finger)\n",
    "    \n",
    "    it_face = datagen_face.flow(data_aug_face,batch_size=num_data_augment,shuffle=False)\n",
    "    it_finger = datagen_finger.flow(data_aug_finger,batch_size=num_data_augment,shuffle=False)\n",
    "    \n",
    "    results_face = np.concatenate([data_face,it_face.next()],axis=0)\n",
    "    results_finger = np.concatenate([data_finger,it_finger.next()],axis=0)\n",
    "    #np.random.shuffle(results)\n",
    "    return results_face,results_finger\n",
    "\n",
    "def augment_data_only_fingerprint(data_finger, num_augment_percent = 0.5):\n",
    "    \n",
    "    num_data = data_finger.shape[0]\n",
    "    num_data_augment = int(data_finger.shape[0]*num_augment_percent)\n",
    "    index_batch = np.arange(0,num_data_augment)\n",
    "    index_data = np.random.choice(num_data,num_data_augment,replace=False)\n",
    "    ###############################\n",
    "    data_aug_finger = []\n",
    "    for i in index_data:\n",
    "        data_aug_finger.append(data_finger[i,:,:,:])   \n",
    "    data_aug_finger = np.asarray(data_aug_finger) \n",
    "    ################################\n",
    "    datagen_finger = ImageDataGenerator(horizontal_flip=False,height_shift_range=0.2,width_shift_range=0.2,shear_range=0.1,rotation_range=20)\n",
    "    datagen_finger.fit(data_aug_finger)\n",
    "    ###############################\n",
    "    it_finger = datagen_finger.flow(data_aug_finger,batch_size=num_data_augment,shuffle=False)\n",
    "    finger_augmented = it_finger.next()\n",
    "    ################################\n",
    "    result_finger = data_finger\n",
    "    for index in zip(index_data,index_batch):\n",
    "        result_finger[index[0],:,:,:]=finger_augmented[index[1],:,:,:]\n",
    "    return result_finger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_face, data_train_finger = augment_data_face_fingerprint(data_train_face,data_train_finger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4284, 256, 256, 3),\n",
       " (4284, 256, 256, 1),\n",
       " (715, 256, 256, 3),\n",
       " (715, 256, 256, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_face.shape,data_train_finger.shape,data_val_face.shape,data_val_finger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_finger = ((data_train_finger)/np.max(data_train_finger))\n",
    "data_val_finger = ((data_val_finger)/np.max(data_val_finger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_finger = np.where(data_train_finger > .5, 1.0, 0.0).astype('float32')\n",
    "data_val_finger = np.where(data_val_finger > .5, 1.0, 0.0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4284, 256, 256, 1), (715, 256, 256, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_finger.shape, data_val_finger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_face = ((data_train_face-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4284, 256, 256, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_face = ((data_val_face-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715, 256, 256, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0 0.0\n",
      "1.0 0.07450981 -1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(data_train_finger),np.median(data_train_finger),np.min(data_train_finger))\n",
    "print(np.max(data_train_face),np.median(data_train_face),np.min(data_train_face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_face = data_train_face\n",
    "val_face = data_val_face\n",
    "train_finger = data_train_finger\n",
    "val_finger = data_val_finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_latentcode,valid_latentcode,train_face,valid_face = train_test_split(data_train_latentcode[2],\n",
    "#                                                              data_train_face,\n",
    "#                                                              test_size=0.2,\n",
    "#                                                              random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4284, 256, 256, 3),\n",
       " (715, 256, 256, 3),\n",
       " (4284, 256, 256, 1),\n",
       " (715, 256, 256, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_face.shape,val_face.shape,train_finger.shape, val_finger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data_train_finger[10],cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow((data_train_face[10]+1)*0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_train/data_get_latent/'\n",
    "with open(os.path.join(DATA_DIR,\"data_face_2856_train.pkl\"), \"rb\") as input_file:\n",
    "    data_train_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_fingerprint_2856_train.pkl\"), \"rb\") as input_file:\n",
    "    data_train_finger = pickle.load(input_file)\n",
    "\n",
    "    \n",
    "with open(os.path.join(DATA_DIR,\"data_face_2856_val.pkl\"), \"rb\") as input_file:\n",
    "    data_val_face = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"data_fingerprint_2856_val.pkl\"), \"rb\") as input_file:\n",
    "    data_val_finger = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_face = data_train_face\n",
    "val_face = data_val_face\n",
    "train_finger = data_train_finger\n",
    "val_finger = data_val_finger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (-1976.351)(R -3934.305, F -18.398)]  [Gan loss: 293.306, Per loss: 293.306,W: 279.281 ]\n",
      "0 [VAE loss: -0.888 (Re -0.874 Mc -0.015) Full 13.167 Per 14.062 Re -0.874 Mc -0.022 ]\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "GAN.train(\n",
    "    train_finger  \n",
    "    , train_face\n",
    "    , val_finger\n",
    "    , val_face\n",
    "    , batch_size = 16\n",
    "    , epochs = 20000\n",
    "    , run_folder = './model_save'\n",
    "    , print_every_n_batches = 10\n",
    "    , n_critic = 2\n",
    "    , using_generator = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN.encoder_model.save('./model_save/encoder_17200_1379.h5')\n",
    "GAN.discriminator.save('./model_save/discriminator_17200_1379.h5')\n",
    "GAN.generator.save('./model_save/generator_17200_1379.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = tf.keras.models.load_model('./model_save/model/generator_3channel_conditional.h5',custom_objects={'get_perceptual_loss': get_perceptual_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(validate=True):    # Test\n",
    "    r, c = 8, 4\n",
    "    if validate :\n",
    "        y_true = val_face\n",
    "        x_true = val_finger\n",
    "    else:\n",
    "        y_true = train_face[900:1000,:,:,:]\n",
    "        x_true = train_finger[900:1000,:,:,:]\n",
    "    \n",
    "    latent_code = GAN.encoder_model.predict(x_true)\n",
    "    gen_imgs = GAN.generator.predict(latent_code[0])\n",
    "    gen_finger = GAN.decoder_model.predict(latent_code[0])\n",
    "    # Perceptual loss\n",
    "    #perceptloss = get_perceptual_loss(y_true,gen_imgs)\n",
    "\n",
    "    indx = np.random.choice(y_true.shape[0], int(0.25*c*r) ,replace=False)\n",
    "    \n",
    "    finger_real = x_true[indx]\n",
    "    \n",
    "    face_real = 0.5*(y_true[indx]+1)\n",
    "\n",
    "    gen_imgs = 0.5 * (gen_imgs[indx] + 1)\n",
    "    gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    " \n",
    "    gen_finger = gen_finger[indx]\n",
    "    print(np.max(gen_finger), np.min(gen_finger),np.median(gen_finger))\n",
    "    fig, axs = plt.subplots(r, c, figsize=(15,30))\n",
    "    #fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(int(0.25*c)):\n",
    "            axs[i,4*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "            axs[i,4*j].axis('off')\n",
    "            axs[i,4*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "            axs[i,4*j+1].axis('off')\n",
    "            axs[i,4*j+2].imshow(np.squeeze(finger_real[cnt, :,:,:]),cmap =\"gray\")\n",
    "            axs[i,4*j+2].axis('off')\n",
    "            axs[i,4*j+3].imshow(np.squeeze(gen_finger[cnt, :,:,:]),cmap =\"gray\")\n",
    "            axs[i,4*j+3].axis('off')\n",
    "            cnt += 1\n",
    "\n",
    "    plt.show()\n",
    "    #fig.savefig(os.path.join('./model_save', \"images/face_gens.png\" ))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAN.sample_images(train_latentcode,train_face,\"./model_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_train/data_split/'\n",
    "with open(os.path.join(DATA_DIR,\"latentcode_val_augment.pkl\"), \"rb\") as input_file:\n",
    "    latentcode_augment = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"face_val_augment.pkl\"), \"rb\") as input_file:\n",
    "    face_augment = pickle.load(input_file)\n",
    "with open(os.path.join(DATA_DIR,\"finger_val_augment.pkl\"), \"rb\") as input_file:\n",
    "    finger_augment = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 128), (300, 128, 128, 3), (300, 128, 128, 1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latentcode_augment[2].shape,face_augment.shape,finger_augment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "finger_augment = ((finger_augment)/np.max(finger_augment))\n",
    "finger_augment = np.where(finger_augment > .5, 1.0, -1.0).astype('float32')\n",
    "#finger_augment = ((finger_augment-127.5)/127.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 8, 4\n",
    "\n",
    "latentcode_datatrain = train_latentcode[200:500,:]\n",
    "y_true_datatrain = train_face[200:500,:,:,:]\n",
    "label_true_datatrain = label_train_finger[200:500,:,:,:]\n",
    "print(latentcode_datatrain.shape)\n",
    "noise_gauss = tf.keras.backend.random_normal(shape=(300, 128))\n",
    "gen_imgs_augment = GAN.generator.predict([latentcode_augment[2]+noise_gauss*0.2,finger_augment])\n",
    "\n",
    "# Perceptual loss\n",
    "perceptloss_augment = get_perceptual_loss(y_true_datatrain,gen_imgs_augment)\n",
    "\n",
    "indx = np.random.choice(y_true_datatrain.shape[0], int(0.5*c*r) ,replace=False)\n",
    "\n",
    "face_real = 0.5*(y_true_datatrain[indx]+1)\n",
    "face_real = face_real[:,:,:,[2,1,0]]\n",
    "\n",
    "gen_imgs = 0.5 * (gen_imgs_augment[indx] + 1)\n",
    "gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "gen_imgs = gen_imgs[:,:,:,[2,1,0]]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15,30))\n",
    "#fig.suptitle(\"Perceptual loss : %.3f\" %(perceptloss_augment))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(int(0.25*c)):\n",
    "        axs[i,4*j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "        axs[i,4*j].axis('off')\n",
    "        axs[i,4*j+1].imshow(np.squeeze(face_real[cnt, :,:,:]))\n",
    "        axs[i,4*j+1].axis('off')\n",
    "        axs[i,4*j+2].imshow(np.squeeze(finger_augment[indx][cnt, :,:,:]),cmap='gray')\n",
    "        axs[i,4*j+2].axis('off')\n",
    "        axs[i,4*j+3].imshow(np.squeeze(label_true_datatrain[indx][cnt, :,:,:]),cmap='gray')\n",
    "        axs[i,4*j+3].axis('off')\n",
    "    cnt += 1\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(os.path.join('./model_save', \"images/face_gens_1.png\" ))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
